#+TITLE: Proof Theory
#+AUTHOR: Gaisi Takeuti

#+EXPORT_FILE_NAME: ../latex/ProofTheory/ProofTheory.tex
#+LATEX_HEADER: \input{../preamble.tex}
#+LATEX_HEADER: \usepackage{ebproof}
#+LATEX_HEADER: \def \LJ {\textbf{LJ}}
#+LATEX_HEADER: \def \LK {\textbf{LK}}
#+LATEX_HEADER: \def \LKp {\textbf{LK'}}
#+LATEX_HEADER: \def \LKe {\textbf{LK}_\textbf{e}}
#+LATEX_HEADER: \def \LJp {\textbf{LJ'}}
#+LATEX_HEADER: \def \LKsh {\textbf{LK\#}}
#+LATEX_HEADER: \def \LKs {\textbf{LK}^*}
#+LATEX_HEADER: \def \Sort {\text{Sort}}
#+LATEX_HEADER: \def \Ex {\text{Ex}}
#+LATEX_HEADER: \def \Un {\text{Un}}
#+LATEX_HEADER: \def \Fr {\text{Fr}}
#+LATEX_HEADER: \def \Pr {\text{Pr}}
#+LATEX_HEADER: \def \Ext {\text{Ext}}
#+LATEX_HEADER: \def \CA {\text{CA}}
#+LATEX_HEADER: \def \VJ {\text{VJ}}
#+LATEX_HEADER: \def \PA {\textbf{PA}}
#+LATEX_HEADER: \def \sb {\text{sb}}
#+LATEX_HEADER: \def \Prov {\text{Prov}}
#+LATEX_HEADER: \def \ovesb {\ove{\sb}}
#+LATEX_HEADER: \def \Consis {\text{Consis}}
#+LATEX_HEADER: \def \oveConsis {\ove{\text{Consis}}}
#+LATEX_HEADER: \def \oveConsisS {\ove{\text{Consis}}_{\bS}}
#+LATEX_HEADER: \newcommand{\ovecor}[1] {\ove{\ucorner{#1}}}
* First Order Predicate Calculus
  <<sec:one>>
  In this chapter we shall present Gentzen's formulation of the first order predicate
  calculus \(\LK\) (logistischer klassischer Kalkül). Intuitionisitic logic is known as \(\LJ\)
  (logistischer intuitionistischer Kalkül)
** Formalization of statements

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   *Terms* are defined inductively as follows:
   1. Every individual constant is a term
   2. Every free variable is a term
   3. If \(f^i\) is a function constant with \(i\) argument-places and \(t_1,\dots,t_i\) are terms,
      then \(f^i(t_1,\dots,t_i)\) is a term
   4. Terms are only those expressions obtained by 1-3.
   #+END_definition
   
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   *Formulas* are defined inductively as:
   3. [@3] If \(A\) is a formula, \(a\) is a free variable and \(x\) is a bound
      variable not occurring in \(A\), then \(\forall xA'\) and \(\exists xA'\) are
      formulas, where \(A'\) is the expression obtained from \(A\) by writing
      \(x\) in place of \(a\) at each occurrence of \(a\) in \(A\)
   #+END_definition

   
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Let \(A\) be an expression, let \(\tau_1,\dots,\tau_n\) be distinct primitive
   symbols, and let \(\sigma_1,\dots,\sigma_n\) be any symbols. By
      \begin{equation*}
   \left(
   A\frac{\tau_1,\dots,\tau_n}{\sigma_1,\dots,\sigma_n}
   \right)
   \end{equation*}
   we mean the expression obtained from \(A\) by writing
   \(\sigma_1,\dots,\sigma_n\) in place of \(\tau_1,\dots,\tau_n\) respectively
   at each occurrence of \(\tau_1,\dots,\tau_n\). Such an operation is called
   the *(simultaneous) replacement of* \((\tau_1,\dots,\tau_n)\) *by*
   \((\sigma_1,\dots,\sigma_n)\) *in* \(A\).

   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   1. If \(A\) contains none of \(\tau_1,\dots,\tau_n\), then
      \begin{equation*}
      \left(
      A\frac{\tau_1,\dots,\tau_n}{\sigma_1,\dots,\sigma_n}
      \right)
      \end{equation*}
      is \(A\) itself
   2. If \(\sigma_1,\dots,\sigma_n\) are distinct primitive symbols, then
      \begin{equation*}
      \left(\left(
      A\frac{\tau_1,\dots,\tau_n}{\sigma_1,\dots,\sigma_n}
      \right)\frac{\sigma_1,\dots,\sigma_n}{\theta_1,\dots,\theta_n}\right)
      \end{equation*}
      is identical with
      \begin{equation*}
      \left(
      A\frac{\tau_1,\dots,\tau_n}{\theta_1,\dots,\theta_n}
      \right)
      \end{equation*} 
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   1. Let \(A\) be a formula and \(t_1,\dots,t_n\) be terms. If there is a
      formula \(B\) and \(n\) distinct free variables \(b_1,\dots,b_n\) s.t.
      \(A\) is
      \begin{equation*}
      \left(
      B\frac{b_1,\dots,b_n}{t_1,\dots,t_n}
      \right)
      \end{equation*}
      then for each \(i(1\le i\le n)\) the occurrences of \(t_1\) resulting from
      the above replacement are said to be *indicated* in \(A\), and this fact is
      also expressed by writing \(B\) as \(B(b_1,\dots,b_n)\) and \(A\) as \(B(t_1,\dots,t_n)\)
   2. A term \(t\) is *fully indicated* in \(A\), or every occurrence of \(t\) in
      \(A\) is indicated, if every occurrence of \(t\) is obtained by such a replacement
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If \(A\) is a formula (where \(a\) is not necessarily fully indicated) and
   \(x\) is a bound variable not occurring in \(A(a)\), then \(\forall xA(x)\) and
   \(\exists xA(x)\) are formulas
   #+END_proposition

   
** Formal proofs and related concepts


   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   For arbitrary \Gamma and \Delta in the above notation, \(\Gamma\to\Delta\) is called a
   *sequent*. \Gamma and \Delta are called the *antecedent* and *succedent*,
   respectively, of the sequent and each formula in \Gamma and \Delta is called a
   *sequent-formula* 
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   label:def2.1
   An *inference* is an expression of the form
   
   \begin{equation*}
   \begin{prooftree}[center=false]
   \hypo{S_1}
   \infer1{S}
   \end{prooftree}\text{ or }
   \begin{prooftree}[center=false]
   \hypo{S_1}
   \hypo{S_2}
   \infer2{S}
   \end{prooftree}
   \end{equation*}
   where \(S_1,S_2\) and \(S\) are sequents. \(S_1\) and \(S_2\) are called the
   *upper sequents* and \(S\) is called the *lower sequent* of the inference

   Structural rules
      1. *Weakening*:
         \begin{equation*}
         \text{left: }
         \begin{prooftree}
         \hypo{\Gamma\to\Delta}
         \infer1{D,\Gamma\to\Delta}
         \end{prooftree};\quad\text{right: }
         \begin{prooftree}
         \hypo{\Gamma\to\Delta}
         \infer1{\Gamma\to\Delta,D}
         \end{prooftree}
         \end{equation*}
         \(D\) is called the *weakening formula*
      2. *Contraction*:
         \begin{equation*}
         \text{left: }
         \begin{prooftree}
         \hypo{D,D,\Gamma\to\Delta}
         \infer1{D,\Gamma\to\Delta}
         \end{prooftree}\quad\text{ right: }
         \begin{prooftree}
         \hypo{\Gamma\to\Delta,D,D}
         \infer1{\Gamma\to\Delta,D}
         \end{prooftree}
         \end{equation*}
      3. *Exchange*
         \begin{equation*}
         \text{left: }
         \begin{prooftree}
         \hypo{\Gamma,C,D,\Pi\to\Delta}
         \infer1{\Gamma,D,C,\Pi\to\Delta}
         \end{prooftree}\quad\text{ right: }
         \begin{prooftree}
         \hypo{\Gamma\to\Delta,C,D,\Lambda}
         \infer1{\Gamma\to\Delta,D,C,\Lambda}
         \end{prooftree}
         \end{equation*}


   We will refer to these three kinds of inferences as "weak inferences",
   while all others will be called "strong inferences"
   4. [@4] *Cut*
      \begin{equation*}
      \begin{prooftree}[center=false]
      \hypo{\Gamma\to\Delta,D}
      \hypo{D,\Pi \to\Lambda}
      \infer2{\Gamma,\Pi\to\Delta,\Lambda}
      \end{prooftree}
      \end{equation*}
      \(D\) is called the *cut formula* of this instance


   Logical rules
   1. 
       \begin{equation*}
       \neg:\text{left: }
       \begin{prooftree}
       \hypo{\Gamma\to\Delta,D}
       \infer1{\neg D,\Gamma\to\Delta}
       \end{prooftree};\quad
       \neg:\text{right: }
       \begin{prooftree} 
       \hypo{D,\Gamma\to\Delta}
       \infer1{\Gamma\to\Delta,\neg D}
       \end{prooftree}
       \end{equation*}
       \(D\) and \(\neg D\) are called the *auxiliary formula* and the *principal
       formula* respectively, of this inference
   2. 
       \begin{align*}
       &
       \begin{prooftree}
       \hypo{C,\Gamma\to\Delta}
       \infer1[\(\wedge\)left]{C\wedge D,\Gamma\to\Delta}
       \end{prooftree}\quad\text{ and }\quad
       \begin{prooftree}
       \hypo{D,\Gamma\to\Delta}
       \infer1[\(\wedge\)left]{C\wedge D,\Gamma\to\Delta}
       \end{prooftree}\\
       &
       \begin{prooftree}[center=false]
       \hypo{\Gamma\to\Delta,C}
       \hypo{\Gamma\to\Delta,D}
       \infer2[\(\wedge\)right]{\Gamma\to\Delta,C\wedge D}
       \end{prooftree}
       \end{align*}
       \(C\) and \(D\) are called the auxiliary formulas and \(C\wedge D\) is
       called the principal formula of this inference

   3. 
       \begin{align*}
       &
       \begin{prooftree}[center=false]
       \hypo{C,\Gamma\to\Delta}
       \hypo{D,\Gamma\to\Delta}
       \infer2[\(\vee\)left]{C\vee D,\Gamma\to\Delta}
       \end{prooftree}\\&
       \begin{prooftree}%[center=false]
       \hypo{\Gamma\to\Delta,C}
       \infer1[\(\vee\)right]{\Gamma\to\Delta,C\vee D}
       \end{prooftree}\quad\text{ and }\quad
       \begin{prooftree}%[center=false]
       \hypo{\Gamma\to\Delta,D}
       \infer1[\(\vee\)right]{\Gamma\to\Delta,C\vee D}
       \end{prooftree}
       \end{align*}
       \(C\) and \(D\) are called the auxiliary formulas and \(C\vee D\) the
       principal formula of this inference
   4.
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{\Gamma\to\Delta,C}
      \hypo{D,\Pi\to \Lambda}
      \infer2[\(\supset\)left]{C\supset D,\Gamma,\Pi\to\Delta,\Lambda}
      \end{prooftree}\hspace{1cm}
      \begin{prooftree}%[center=false]
      \hypo{C,\Gamma\to\Delta,D}
      \infer1[\(\supset\)right]{\Gamma\to\Delta,C\supset D}
      \end{prooftree}
      \end{equation*}
      \(C\) and \(D\) are called the auxiliary formulas and \(C\supset D\) the
      principal formula


   1-4 are called *propositional inferences*
   5. [@5]
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{F(t),\Gamma\to\Delta}
      \infer1[\(\forall\)left]{\forall xF(x),\Gamma\to\Delta}
      \end{prooftree}\hspace{1cm}
      \begin{prooftree}%[center=false]
      \hypo{\Gamma\to\Delta,F(a)}
      \infer1[\(\forall\)right]{\Gamma\to\Delta,\forall xF(x)}
      \end{prooftree}
      \end{equation*}
      where \(t\) is an arbitrary term, and \(a\) does not occur in the lower
      sequent. \(F(t)\) and \(F(a)\) are called the auxiliary formulas and
      \(\forall xF(x)\) the principal formula. The \(a\) in \(\forall\)right is called
      the *eigenvariable* of this inference


   In \(\forall\)right all occurrences of \(a\) in \(F(a)\) are indicated. In
   \(\forall\)left, \(F(t)\) and \(F(x)\) are
   \begin{equation*}
   \left(F(a)\frac{a}{t}
   \right)\quad\text{ and }\quad
   \left(F(a)\frac{a}{t}
   \right)
   \end{equation*}
   respectively, so not every \(t\) in \(F(t)\) is necessarily indicated

   6. [@6]
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{F(a),\Gamma\to\Delta}
      \infer1[\(\exists\)left]{\exists xF(x),\Gamma\to\Delta}
      \end{prooftree}\hspace{1cm}
      \begin{prooftree}%[center=false]
      \hypo{\Gamma\to\Delta,F(t)}
      \infer1[\(\exists\)right]{\Gamma\to\Delta,\exists xF(x)}
      \end{prooftree}
      \end{equation*}
      where \(a\) does not occur in the lower sequent, and \(t\) is an arbitrary
      term

      \(F(a)\) and \(Ft\) are called the auxiliary formulas and \(\exists xF(x)\) the
      principal formula. The \(a\) in \(\exists\)left is called the
      eigenvariable of this inference


   In \(\exists\)left \(a\) is fully indicated

   5 and 6 are called the *quantifier inferences*. The condition, that the
   eigenvariable must not occur in the lower sequent in \(\forall\)right and
   \(\exists\)left is called the *eigenvariable condition*

   A sequent of the form \(A\to A\) is called an *initial sequent* or axiom
   #+END_definition   

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A *proof* \(P\) (in \(\LK\)), or *\(\LK\)-proof*, is a tree of sequents
   satisfying the following conditions
   1. The topmost sequents of \(P\) are initial sequents
   2. Every sequent in \(P\) except the lowest one is an upper sequent of an
      inference whose lower sequent is also in \(P\)
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   1. A sequence of sequents in a proof \(P\) is called a *thread* (of \(P\)) if
      the following conditions are satisfied
      1. The sequence begins with an initial sequent and ends with the end-sequent
      2. Every sequent in the sequence except the last is an upper sequent of an
         inference, and is immediately followed by the lower sequent of this inference
   2. Let \(S_1,S_2\)and \(S_3\) be sequents in a proof \(P\). We say \(S_1\)
         is *above* \(S_2\)or \(S_2\) is *below* \(S_1\) if there is a thread
         containing both \(S_1\)and \(S_2\) where \(S_1\) appears before
         \(S_2\). If \(S_1\)is above \(S_2\)and \(S_2\) is above \(S_3\), we say
         \(S_2\) is *between* \(S_1\) and \(S_3\)
   3. An inference in \(P\) is said to be *below a sequent* \(S\) if its lower
      sequent is below \(S\)
   4. Let \(P\) be a proof. A part of \(P\) which itself is a proof is called a
      *subproof* of \(P\). For any sequent \(S\) in \(P\), that part of \(P\)
      which consists of all sequents which are either \(S\)itself or which occur
      above \(S\)is called a subproof of \(P\) (with end-sequent \(S\))
   5. Let \(P_0\) be a proof of the form
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{}
      \ellipsis{}{\Gamma\to\Theta}
      \ellipsis{(*)}{}
      \end{prooftree}
      \end{equation*}
      where (*) denotes the part of \(P_0\) under \(\Gamma\to\Theta\), and let
      \(Q\) be a proof ending with \(\Gamma,D\to\Theta\). By a copy of \(P_0\) from
      \(Q\) we mean a proof \(P\) of the form
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{}
      \ellipsis{Q}{\Gamma,D\to\Theta}
      \ellipsis{(**)}{}
      \end{prooftree}
      \end{equation*}
      where \((**)\) differs from \((*)\) only in that for each sequent in \((*)\),
      say \(\Gamma\to\Lambda\), the corresponding sequent in \((**)\) has the
      form \(\Pi,D\to\Lambda\).
   6. Let \(S(a)\) or \(\Gamma(a)\to\Delta(a)\), denote a sequent of the form
      \(A_1(a),\dots,A_m(a)\to B_1(a),\dots,B_n(a)\). Then \(S(t)\), or
      \(\Gamma(t)\to\Delta(t)\), denotes the sequent
   \(A_1(t),\dots,A_m(t)\to B_1(t),\dots,B_n(t)\)
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A proof in \(\LK\) is called *regular* if it satisfies the condition that all
   eigenvariables are distinct from one another and if a free variable \(a\)
   occurs as an eigenvariable in a sequent \(S\) of the proof, then \(a\) occurs
   only in sequents above \(S\)
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   label:lemma2.9
   1. Let \(\Gamma(a)\to\Delta(a)\) be an (\(\LK\)-)provable sequent in which \(a\)
      is fully indicated, and let \(P(a)\) be a proof of \(\Gamma(a)\to\Delta(a)\).
      Let \(b\) be a free variable not occurring in \(P(a)\). Then the tree
      \(P(b)\), obtained from \(P(a)\) by replacing \(a\) by \(b\) at each
      occurrence of \(a\) in \(P(a)\), is also a proof and its end-sequent is \(\Gamma(b)\to\Delta(b)\)
   2. For an arbitrary \(\LK\)-proof there exists a regular proof of the same
      end-sequent. Moreover, the required proof is obtained from the original
      proof simply by replacing free variables
   #+END_lemma

   #+BEGIN_proof
   1. By induction on the number of inference in \(P(a)\). If \(P(a)\) consists
      of simply an initial sequent \(A(a)\to A(a)\), then \(P(b)\) consists of the
      sequent \(A(b)\to A(b)\).

      Suppose that our proposition holds for proofs containing at most \(n\)
      inferences and suppose that \(P(a)\) contains \(n+1\) inferences. We treat
      the possible cases according to the last inferences in \(P(a)\). Since
      other cases can be treated similarly, we consider only the case where the
      last inference, say \(J\), is a \(\forall\)right. Suppose the
      eigenvariable of \(J\) is \(a\), and \(P(a)\) is of the form
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{}
      \ellipsis{\(Q(a)\)}{\Gamma\to\Lambda,A(a)}
      \infer1[\(J\)]{\Gamma\to\Lambda,\forall xA(x)}
      \end{prooftree}
      \end{equation*}
      where \(Q(a)\) is the subproof of \(P(a)\) ending with
      \(\Gamma\to\Lambda,A(a)\). \(a\) doesnt occur in \Gamma, \Lambda or \(A(x)\). By the
      induction hypotheses the result of replacing all \(a\)'s  in \(Q(a)\) by
      \(b\) is a proof whose end-sequent is \(\Gamma\to\Lambda,A(b)\). \Gamma and \Lambda
      contain no \(b\)'s. Thus we can apply a \(\forall\)right to this sequent
      using \(b\) as its eigenvariable
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{}
      \ellipsis{\(Q(b)\)}{\Gamma\to\Lambda,A(b)}
      \infer1[]{\Gamma\to\Lambda,\forall xA(x)}
      \end{prooftree}
      \end{equation*}
      and so \(P(b)\) is a proof ending with \(\Gamma\to\Lambda,\forall xA(x)\). If
      \(a\) is not the eigenvariable of \(J\), \(P(a)\) is of the form
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{}
      \ellipsis{\(Q(a)\)}{\Gamma(a)\to\Lambda(a),A(a,c)}
      \infer1{\Gamma(a)\to\Lambda(a),\forall xA(a,x)}
      \end{prooftree}
      \end{equation*}
      By the induction hypothesis the result of replacing all \(a\)'s in
      \(Q(a)\) by \(b\)is a proof and its end-sequent is
      \(\Gamma(b)\to\Lambda(b),A(b,c)\)

      Since by assumption \(b\) doesn't occur in \(P(a)\), \(b\) is not \(c\)
      and so we can apply a \(\forall\)right to this sequent, with \(c\) as its eigenvariable

   2. By mathematical induction on the number \(l\) of applications of
      \(\forall\)right and \(\exists\)left in a given proof \(P\). If \(l=0\)
      then take \(P\) itself. Otherwise, \(P\) can be represented in the form
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{P_1\quad P_2\dots P_k}
      \ellipsis{(*)}{S}
      \end{prooftree}
      \end{equation*}
      where \(P_i\) is a subproof of \(P\) of the form
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{}
      \ellipsis{}{\Gamma_i\to\Delta_i,F_i(b_i)}
      \infer1[\(I_i\)]{\Gamma_i\to\Delta_i,\forall y_iF_i(y_i)}
      \end{prooftree}
      \quad\text{ or }\quad
      \begin{prooftree}%[center=false]
      \hypo{}
      \ellipsis{}{F_i(b_i),\Gamma_i\to\Delta_i}
      \infer1[\(I_i\)]{\exists y_iF_i(y_i),\Gamma_i\to\Delta_i}
      \end{prooftree}      
      \end{equation*}
      and \(I_i\) is a lowermost \(\forall\)right or \(\exists\)left in \(P\)

      Let us deal with the case where \(I_i\) is \(\forall\)right. \(P_i\) has
      fewer applications of \(\forall\)right or \(\exists\)left than \(P\), so
      by the induction hypothesis there is a regular proof \(P_i'\) of
      \(\Gamma_i\to\Delta_i,F_i(b_i)\). Note that no free variable in
      \(\Gamma_i\to\Delta_i,F_i(b_i)\) (including \(b_i\)) is used as an
      eigenvariable in \(P_i'\). Suppose \(c_1,\dots,c_m\) are all the
      eigenvariables in all the \(P_i\)'s which occur in \(P\) above
      \(\Gamma_i\to\Delta_i,\forall y_iF_i(y_i), i=1,\dots,k\). Then change
      \(c_1,\dots,c_m\) to \(d_1,\dots,d_m\) respectively, where
      \(d_1,\dots,d_m\) are the first \(m\) variables which occur neither in
      \(P\) nor in \(P_i\)'. If \(b_i\) occurs in \(P\) below
      \(\Gamma_i\to\Delta_i,\forall y_iF_i(y_i)\) then change it to \(d_{m+i}\)

      Let \(P_i''\) be the proof which is obtained from \(P_i'\) by the above
      replacement of varaibles. Then \(P_1'',\dots,P_k''\) are each regular
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{P_1''\dots
      \begin{prooftree}%[center=false]
      \hypo{P_i''}
      \infer1{\Gamma_i\to\Delta_i,\forall y_iF_i(y_i)}
      \end{prooftree}\dots P_n''}
      \ellipsis{(*)}{S}
      \end{prooftree}
      \end{equation*}
   #+END_proof

   From now on we will assume that we are dealing with regular proofs whenever
   convenient

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   label:lemma2.11
   Let \(t\) be an arbitrary term. Let \(\Gamma(a)\to \Delta(a)\) be a provable (in \(\LK\))
   sequent in which \(a\) is fully indicated, and let \(P(a)\) be a proof ending
   with \(\Gamma(a)\to \Delta(a)\) in which *every eigenvariable is different from \(a\) and
   not contained in \(t\)*. Then \(P(t)\) is a proof whose end-sequent is
   \(\Gamma(t)\to \Delta(t)\)
   #+END_lemma

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   Let \(t\) be an arbitrary term. Let \(\Gamma(a)\to \Delta(a)\) be a provable (in \(\LK\))
   sequent in which \(a\) is fully indicated, and let \(P(a)\) be a proof of
   \(\Gamma(a)\to \Delta(a)\). Let \(P'(a)\) be a proof obtained from \(P(a)\) by changing
   eigenvariables in such a way that in \(P'(a)\) every eigenvariable is
   different from \(a\) and not contained in \(t\). Then \(P'(t)\) is a proof of
   \(\Gamma(t)\to \Delta(t)\)
   #+END_lemma

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(t\) be an arbitrary term and \(S(a)\) a provable sequent in which \(a\)
   is fully indicated. Then \(S(t)\) is also provable
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop2.14
   If a sequent is provable, then it is provable with a proof in which all the
   initial sequents consist of atmoic formulas. Furthermore, if a sequent is
   provable without cut, then it is provable without cut with a proof of the
   above sort
   #+END_proposition

   #+BEGIN_proof
   It suffices to show that for an arbitrary formula \(A\), \(A\to A\) is
   provable without cut, starting with initial sequents consisting of atomic formulas.
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Two formulas \(A\) and \(B\) are *alphabetical variants* if for some
   \(x_1,\dots,x_n,y_1,\dots,y_n\)
   \begin{equation*}
   \left(A\frac{x_1,\dots,x_n}{z_1,\dots,z_n}
   \right)
   \end{equation*}
   is
   \begin{equation*}
   \left(
   B\frac{y_1,\dots,y_n}{z_1,\dots,z_n}
   \right)
   \end{equation*}
   where \(z_1,\dots,z_n\) are bound variables occurring neither in \(A\) nor in
   \(B\). The fact that \(A\) and \(B\) are alphabetical variants will be
   expressed by \(A\sim B\)
   #+END_definition

** A formulation of intuitionistic predicate calculus
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   We can formalize the intuitionistic predicate calculus as a subsystem of
   \(\LK\) which we call \(\LJ\) following Gentzen (\(\bJ\) stands for
   "intuitionistic"). \(\LJ\)is obtained from \(\LK\) by modifying it as follows
   1. A sequent in \(\LJ\) is of the form \(\Gamma \to \Delta \) where \Delta consists of at most
      one formula
   2. Inferences in \(\LJ\) are those obtained from those in \(\LK\) by imposing
      the restriction that the succedent of each upper and lower sequent
      consists of at most one formula; thus there are no inferences in
      \(\LJ\)corresponding to contraction right or exchange right
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If a sequent \(S\) of \(\LJ\) is provable in \(\LJ\), then it is also
   provable in \(\LK\)
   #+END_proposition

** Axiom systems
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   The basic system is \(\LK\)
   1. A finite or infinite set \(\cala\) of sentences is called an *axiom system*,
      and each of these sentences is called an *axiom* of \(\cala\). Sometimes an
      axiom system is called a *theory*
   2. A finite (possibly empty) sequence of formulas consisting only of axioms
      of \(\cala\) is called an *axiom sequence* of \(\cala\)
   3. If there exists an axiom sequence \(\Gamma_0\) of \(\cala\) s.t.
      \(\Gamma_0,\Gamma\to\Delta\) is \(\LK\)-provable, then \(\Gamma \to \Delta\) is
      said to be *provable from* \(\cala\) (in \(\LK\)). We express this by \(\cala,\Gamma\to\Delta\)
   4. \(\cala\) is *inconsistent* (with \(\LK\)) if the empty sequent \(\to\) is
      provable from \(\cala\) (in \(\LK\))
   5. If all function constants and predicate constants in a formula \(A\) occur
      in \(\cala\), then \(A\) is said to be *dependent on* \(\cala\)
   6. A sentence \(A\) is *consistent* if the axiom system \(\{A\}\) is consistent
   7. \(\LK_{\cala}\) is the system obtained from \(\LK\) by adding \(\to A\) as
      initial sequents for all \(A\) in \(\cala\)
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop4.2
   Let \(\cala\) be an axiom system. Then the following are equivalent
   1. \(\cala\) is inconsistent (with \(\LK\))
   2. for every formula \(A\), \(A\) is provable from \(\cala\)
   3. for some formula \(A\), \(A\)and \(\neg A\) are both provable from \(\cala\)
   #+END_proposition

   #+BEGIN_proof
   \(3\to1\). we have \(\LK\vdash A\leftrightarrow\neg\neg A\). So from \(\to\neg A\) we
   have \(A\to\). THen we apply cut.
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(\cala\) be an axiom system. Then a sequent \(\Gamma\to \Delta\) is
   \(\LK_{\cala}\)-provable iff \(\Gamma\to\Delta\) is provable from \(\cala\)
   (in \(\LK\))
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   An axiom system \(\cala\) is consistent (with \(\LK\)) iff \(\LK_{\cala}\) is consistent
   #+END_corollary

   These definitions and the propositions hold also for \(\LJ\)

** The cut-elimination theroem
   #+ATTR_LATEX: :options [the cut-elimination theroem: Gentzen]
   #+BEGIN_theorem
   label:thm5.1
   If a sequent is \((\LK)\)-provable, then it is \((\LK)\)-provable without a cut
   #+END_theorem

   Let \(A\)be a formula. An inference of the following form is called a *mix*
   (w.r.t. \(A\)):
   \begin{equation*}
   \begin{prooftree}%[center=false]
   \hypo{\Gamma\to\Delta}
   \hypo{\Pi\to\Lambda}
   \infer2[\(A\)]{\Gamma,\Pi^* \to \Delta^*, \Lambda}
   \end{prooftree}
   \end{equation*}
   where both \Delta and \Pi contain the formula \(A\), and \(\Delta^*\) and \(\Pi^*\) are
   obtained from \Delta and \Pi respectively by deleting all the occurrences of \(A\)
   in them. We call \(A\) the mix formula of this inference.

   Let's call the system which is obtained from \(\LK\) by replacing the cut
   rule by the mix rule, \(\LKs\).

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   \(\LK\) and \(\LKs\) are equivalent, that is, a sequent \(S\) is
   \(\LK\)-provable iff \(S\) is \(\LKs\)-provable
   #+END_lemma

   mix is a strengthened version of cut

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   If a sequent is provable in \(\LKs\), then it's provable in \(\LKs\) without
   a mix
   #+END_theorem

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   If \(P\) is a proof of \(S\) (in \(\LKs\)) which contains (only) one mix,
   occurring as the last inference, then \(S\) is provable without a mix
   #+END_lemma

   The *grade* of a formula \(A\) (denoted by \(g(A)\)) is the number of logical
   symbols contained in \(A\). The grade of a mix is the grade of the mix
   formula. When a proof \(P\) has a mix as the last inference, we define the
   grade of \(P\) (denoted by \(g(P)\)) to be the grade of this mix.

   Let \(P\) be a proof which contains  a mix only as the last inference
   \begin{equation*}
   J\;\begin{prooftree}%[center=false]
   \hypo{\Gamma\to\Delta}
   \hypo{\Pi \to \Lambda}
   \infer2[\((A)\)]{\Gamma,\Pi^* \to \Delta^*,\Lambda}
   \end{prooftree}
   \end{equation*}
   We refer to the left and right upper sequents as \(S_1\) and \(S_2\) and 
   the lower sequent as \(S\). We call a thread in \(P\) a *left (right) thread*
   if it contains the left (right) upper sequent of the mix \(J\). The *rank* of a
   thread \(\calf\) in \(P\) is defined as follows: if \(\calf\) is a left
   (right) thread, then the rank of \(\calf\) is the number consecutive
   sequents, counting upward from the left (right) upper sequent of \(J\), that
   contains the mix formula in its succedent (antecedent). The rank of a thread
   \(\calf\) in \(P\) is denoted by \(\rank(\calf;P)\). We define
   \begin{equation*}
   \rank_l(P)=\max_{\calf}(\rank(\calf;P))
   \end{equation*}
   where \(\calf\) ranges over all the left threads in \(P\), and
   \begin{equation*}
   \rank_r(P)=\max_{\calf}(\rank(\calf;P))
   \end{equation*}
   where \(\calf\) ranges over all the right threads in \(P\). The rank of
   \(P\), \(\rank(P)\), is defined as
   \begin{equation*}
   \rank(P)=\rank_l(P)+\rank_r(P)
   \end{equation*}
   Note that \(\rank(P)\ge 2\)
   
   #+BEGIN_proof
   We prove the Lemma by double induction on the grade \(g\) and rank \(r\) of
   the proof \(P\) (i.e. transfinite induction on \(\omega\cdot g+r\)). We
   divide the proof into two main cases, namely \(r=2\) and \(r>2\)

   1. \(r=2\), \(\rank_l(P)=\rank_r(P)=1\)
      1. The left upper sequent \(S_1\) is an initial sequent. In this case we
         may assume \(P\) is of the form
         \begin{equation*}
         J\;\begin{prooftree}%[center=false]
         \hypo{A\to A}
         \hypo{\Pi\to \Lambda}
         \infer2{A,\Pi^*\to\Lambda}
         \end{prooftree}
         \end{equation*}
         We can obtain the lower sequent without a mix
         \begin{equation*}
         \begin{prooftree}%[center=false]
         \hypo{\Pi\to\Lambda}
         \infer1{\text{some exchanges}}
         \infer1{A,\dots,A,\Pi^*\to\Lambda}
         \infer1{\text{some contractions}}
         \infer1{A,\Pi^*\to\Lambda}
         \end{prooftree}
         \end{equation*}
      2. The right upper sequent \(S_2\) is an initial sequent.
      3. Neither \(S_1\) nor \(S_2\) is an initial sequent, and \(S_1\) is the
         lower sequent of a structural inference \(J_1\). Since
         \(\rank_l(P)=1\), the formula \(A\) cannot appear in the succedent of
         the upper sequent of \(J_1\). Hence
         \begin{equation*}
         \begin{prooftree}%[center=false]
         \hypo{\Gamma\to\Delta_1}
         \infer1[\(J_1\)]{\Gamma\to\Delta_1,A}
         \hypo{\Pi\to\Lambda}
         \infer2[\(J\)]{\Gamma,\Pi^*\to\Delta_1,\Lambda}
         \end{prooftree}
         \end{equation*}
         where \(\Delta_1\) doesn't contain \(A\). We can eliminate the mix as
         follows
         \begin{equation*}
         \begin{prooftree}%[center=false]
         \hypo{\Gamma\to\Delta_1}
         \infer1{\text{some weakenings}}
         \infer1{\Pi^*,\Gamma\to\Delta_1,\Lambda}
         \infer1{\text{some exchanges}}
         \infer1{\Gamma,\Pi^*\to\Delta_1,\Lambda}
         \end{prooftree}
         \end{equation*}
      4. None of 1.1-1.3 holds but \(S_2\) is the lower sequent of a structural
         inference. Similarly
      5. Both \(S_1\)and \(S_2\)are the lower sequents of logical inferences. In
         this case, since \(\rank_l(P)=\rank_r(P)=1\), the mix formula on each
         side must be the principal formula of the logical inference. We use
         induction on the grade, distinguishing several cases according to the
         outermost logical symbol of \(A\)
         1. The outermost logical symbol of \(A\) is \(\wedge\)
            \begin{equation*}
            \begin{prooftree}%[center=false]
            \hypo{\Gamma\to\Delta_1,B}
            \hypo{\Gamma\to\Delta_1,C}
            \infer2{\Gamma\to\Delta_1,B\wedge C}
            \hypo{B,\Pi_1\to\Lambda}
            \infer1{B\wedge C,\Pi_1\to \Lambda}
            \infer2[(\(B\wedge C\))]{\Gamma,\Pi_1\to\Delta_1,\Lambda}
            \end{prooftree}
            \end{equation*}
            where by assumption none of the proofs ending with
            \(\Gamma\to\Delta_1,B\);\(\Gamma\to\Delta_1,C\) or
            \(B,\Pi_1\to\Lambda\) contain a mix. Consider the following
            \begin{equation*}
            \begin{prooftree}%[center=false] 
            \hypo{\Gamma\to\Delta_1,B}
            \hypo{B,\Pi_1\to\Lambda}
            \infer2[\((B)\)]{\Gamma,\Pi_1^\#\to\Delta_1^\#,\Lambda}
            \end{prooftree}
            \end{equation*}
            This proof contains only one mix, a mix that occurs as its last
            inference. Furthermore the grade of the mix formula \(B\) is less 
            than \(g(A)\). So by induction hypothesis we can obtain a proof
            which contains no mixes and whose end-sequent is
            \(\Gamma,\Pi_1^\#\to\Delta_1^\#,\Lambda\). From this we can obtain a proof
            without a mix with end-sequent \(\Gamma,\Pi_1\to\Delta_1,\Lambda\)
         2. The outermost logical symbol of \(A\) is \(\vee\). Similar.
         3. The outermost logical symbol of \(A\) is \(\forall\) company
            \begin{equation*}
            \begin{prooftree}%[center=false]
            \hypo{\Gamma\to\Delta_1,F(a)}
            \infer1{\Gamma\to\Delta_1,\forall xF(x)}
            \hypo{F(t),\Pi_1\to\Lambda} 
            \infer1{\forall xF(x),\Pi_1\to\Lambda}
            \infer2{\Gamma,\Pi_1\to\Delta_1,\Lambda}
            \end{prooftree}
            \end{equation*}
            (\(a\) being fully indicated in \(F(a)\)). By the eigenvariable
            condition, \(a\) does not occur in \(\Gamma,\Delta_1\) or \(F(x)\). Since
            by assumption the proof ending with \(\Gamma\to\Delta_1, F(a)\)
            contains no mix, we can obtain a proof without a mix, ending with
            \(\Gamma\to\Delta_1,F(t)\). Consider 
            \begin{equation*}
            \begin{prooftree}%[center=false]
            \hypo{\Gamma\to\Delta_1,F(t)}
            \hypo{F(t),\Pi_1\to\Lambda}
            \infer2[\((F(t))\)]{\Gamma,\Pi_1^\#\to\Delta_1^\#,\Lambda}
            \end{prooftree}
            \end{equation*}
         4. The outermost logical symbol of \(A\) is \(\exists\). Similar.
         5. The outermost logical symbol of \(A\) is \(\neg\).
            Then the end of the derivation runs
            \begin{equation*}
            \begin{prooftree}%[center=false]
            \hypo{A,\Gamma\to\Delta_1}
            \infer1{\Gamma\to\Delta_1,\neg A}
            \hypo{\Pi_1\to\Lambda,A}
            \infer1{\neg A,\Pi_1\to\Lambda}
            \infer2{\Gamma,\Pi_1\to\Delta_1,\Lambda}
            \end{prooftree}
            \end{equation*}
            This is transformed into
            \begin{equation*}
            \begin{prooftree}%[center=false]
            \hypo{\Pi_1\to\Lambda,A}
            \hypo{A,\Gamma \to\Delta_1}
            \infer2{\Pi_2\to\Gamma^\#\to\Lambda^\#,\Delta_1}
            \infer[double]1{\Gamma,\Pi_1\to\Delta_1,\Lambda}
            \end{prooftree}
            \end{equation*}
         6. The outermost logical symbol of \(A\) is \(\supset\).
            \begin{equation*}
            \begin{prooftree}%[center=false]
            \hypo{C,\Gamma_1\to\Delta_1,D}
            \infer1{\Gamma_1\to\Delta_1,C\supset D}
            \hypo{\Gamma\to\Delta,C}
            \hypo{D,\Pi\to\Lambda}
            \infer2{C\supset D,\Gamma,\Pi\to\Delta,\Lambda}
            \infer2{\Gamma_1,\Gamma,\Pi \to\Delta_1,\Delta,\Lambda}
            \end{prooftree}
            \end{equation*}
            This is transformed into
            \begin{equation*}
            \begin{prooftree}%[center=false]
            \hypo{\Gamma\to\Delta,C}
            \hypo{C,\Gamma_1\to\Delta_1,D}
            \hypo{D,\Pi\to\Lambda}
            \infer2{C,\Gamma_1,\Pi^\#\to\Delta_1^\#,\Lambda}
            \infer2{\Gamma,\Gamma_1^\#,\Pi^{\#\#}\to\Delta^\#,\Delta_1^\#,\Lambda}
            \infer[double]1{\Gamma_1,\Gamma,\Pi\to\Delta_1,\Delta,\Lambda}
            \end{prooftree}
            \end{equation*}
   2. \(r>2\), i.e., \(\rank_l(P)>1\) and/or \(\rank_r(P)>1\)
            \begin{equation*}
            \begin{prooftree}%[center=false]
            \hypo{A,\Gamma\to\Delta_1}
            \infer1{\Gamma\to\Delta_1,\neg A}
            \hypo{\Pi_1\to\Lambda,A}
            \infer1{\neg A,\Pi_1\to\Lambda}
            \infer2{\Gamma,\Pi_1\to\Delta_1,\Lambda}
            \end{prooftree}
            \end{equation*}
            This is transformed into
            \begin{equation*}
            \begin{prooftree}%[center=false]
            \hypo{\Pi_1\to\Lambda,A}
            \hypo{A,\Gamma \to\Delta_1}
            \infer2{\Pi_2\to\Gamma^\#\to\Lambda^\#,\Delta_1}
            \infer[double]1{\Gamma,\Pi_1\to\Delta_1,\Lambda}
            \end{prooftree}
            \end{equation*}
      We distinguish two main cases: The right rank is greater than 1 and the right rank is equal to 1

      The induction hypothesis is that every proof \(Q\) which contains a mix
      only as the last inference, and which satisfies either \(g(Q)<g(P)\), or
      \(g(Q)=g(P)\) and \(\rank(Q)<\rank(P)\), we can eliminate the mix
      1. \(\rank_r(P)>1\)

         1. \Gamma or \Delta (in \(S_1\)) contains \(A\). Construct a proof as follows
            \begin{equation*}
            \begin{prooftree}%[center=false]
            \hypo{}
            \ellipsis{}{\Pi\to\Lambda}
            \infer1{\text{exchanges/contractions}}
            \infer1{A,\Pi^*\to\Lambda}
            \infer1{\text{weakenings/exchanges}}
            \infer1{\Gamma,\Pi^*\to\Delta^*,\Lambda}
            \end{prooftree}\quad
            \begin{prooftree}%[center=false]
            \hypo{}
            \ellipsis{}{\Gamma\to\Delta}
            \infer1{\text{exchanges/contractions}}
            \infer1{\Gamma\to\Delta^*,A}
            \infer1{\text{weakenings/exchanges}}
            \infer1{\Gamma,\Pi^*\to\Delta^*,\Lambda}
            \end{prooftree}
            \end{equation*}

         2. \(S_2\) is the lower sequent of an inference \(J_2\), where \(J_2\)
            is not a logical inference whose principal formula is \(A\). The
            last part of \(P\) looks like this
            \begin{equation*}
            \begin{prooftree}%[center=false]
            \hypo{\Gamma\to\Delta}
            \hypo{\Phi\to\Psi}
            \infer1[\(J_2\)]{\Pi\to\Lambda}
            \infer2{\Gamma,\Pi^*\to\Delta^*,\Lambda}
            \end{prooftree}
            \end{equation*}
            where the proofs \(\Gamma\to\Delta\) and \(\Phi\to\Psi\) contain no
            mixes and \Phi contains at least one \(A\). Consider the following
            proof \(P'\):
            \begin{equation*}
            \begin{prooftree}%[center=false]
            \hypo{\Gamma\to\Delta}
            \hypo{\Phi\to\Psi}
            \infer2[(\(A\))]{\Gamma,\Phi^*\to\Delta^*,\Psi}
            \end{prooftree}
            \end{equation*}
            In \(P'\), the grade of the mix is equal to \(g(P)\),
            \(\rank_l(P')=\rank_l(P)\) and \(\rank_r(P')=\rank_r(P)-1\). Thus by
            induction hypothesis, \(\Gamma,\Phi^*\to\Delta^*,\Psi\) is provable without
            a mix. Then we construct the proof 
            \begin{equation*}
            \begin{prooftree}%[center=false]
            \hypo{\Gamma,\Phi^*\to\Delta^*,\Psi}
            \infer1{\text{some exchanges}}
            \infer1{\Phi^*,\Gamma\to\Delta^*,\Psi}
            \infer1[\(J_2\)]{\Pi^*,\Gamma\to\Delta^*,\Lambda}
            \end{prooftree}
            \end{equation*}

         3. \Gamma contains no \(A\)'s and \(S_2\) is the lower sequent of a logical
            inference whose principal formula is \(A\).
            1. \(A\) is \(B\supset C\). The last part of \(P\) is of the form
               \begin{equation*}
               \begin{prooftree}%[center=false]
               \hypo{\Gamma\to\Delta}
               \hypo{\Pi_1\to\Lambda_1,B}
               \hypo{C,\Pi_2\to\Lambda_2}
               \infer2{B\supset C,\Pi_1,\Pi_2\to\Lambda_1,\Lambda_2}
               \infer2{\Gamma,\Pi^*_1,\Pi_2^*\to\Delta^*,\Lambda_1,\Lambda_2}
               \end{prooftree}
               \end{equation*}
               Consider the following proofs \(P_1\) and \(P_2\)
               \begin{equation*}
               \begin{prooftree}%[center=false]
               \hypo{\Gamma\to\Delta}
               \hypo{\Pi_1\to\Lambda_1,B}
               \infer2[\(B\supset C\)]{\Gamma_1^*\to\Delta^*,\Lambda_1,B}
               \end{prooftree}
               \hspace{1cm}
               \begin{prooftree}%[center=false]
               \hypo{\Gamma\to\Delta}
               \hypo{C,\Pi_2\to\Lambda_2\to\Lambda_2}
               \infer2[\(B\supset C\)]{\Gamma,C,\Pi_2^*\to\Delta^*,\Lambda_2}
               \end{prooftree}
               \end{equation*}
               assuming that \(B\supset C\) is in \(\Pi_1\) and \(\Pi_2\). If \(B\supset C\) is not
               in \(\Pi_i\) (\(i=1\) or 2), then \(\Pi_i^*\) is \(\Pi_i\) and \(P_i\) is defined as
               \begin{equation*}
                \begin{prooftree}%[center=false]
                \hypo{\Pi_1\to\Lambda_1,B}
                \infer[double]1{\Gamma,\Pi_1^*\to\Delta^*,\Lambda_1,B}
                \end{prooftree}
                \hspace{1cm}
                \begin{prooftree}%[center=false]
                \hypo{C,\Pi_2\to\Lambda_2}
                \infer[double]1{\Gamma,C,\Pi_2^*\to\Delta^*,\Lambda_2}
                \end{prooftree}
               \end{equation*}
               Note that \(g(P_1)=g(P_2)=g(P)\), \(\rank_l(P_1)=\rank_l(P_2)=\rank_l(P)\) and
               \(\rank_r(P_1)=\rank_r(P_2)=\rank_r(P)-1\). Hence by the induction hypothesis, the
               end-sequents of \(P_1\) and \(P_2\) are provable without a mix (say by \(P_1'\)
               and \(P_2'\)). Consider the following proof \(P'\)
               \begin{equation*}
               \begin{prooftree}%[center=false]
               \hypo{\Gamma\to\Delta}
               \hypo{}
               \ellipsis{\(P_1'\)}{\Gamma,\Pi_1^*\to\Delta^*,\Lambda_1,B}
               \hypo{}
               \ellipsis{\(P_2'\)}{\Gamma,C,\Pi_2^*\to\Delta^*,\Lambda_2}
               \infer[double]1{C,\Gamma,\Pi_2^*\to\Delta^*,\Lambda_2}
               \infer2{B\supset C,\Gamma,\Pi_1^*,\Gamma,\Pi_2^*\to\Delta^*,\Lambda_1,
               \Delta^*,\Lambda_2}
               \infer2[\(B\supset C\)]{\Gamma,\Gamma,\Pi_1^*,\Gamma,\Pi_2^*\to\Delta^*,
               \Delta^*,\Lambda_1,\Delta^*,\Lambda_2}
               \end{prooftree}
               \end{equation*}
               Then \(g(P')=g(P)\), \(\rank_l(P')=\rank_l(P)\), \(\rank_r(P')=1\). Thus the
               end-sequent of \(P'\) is provable without a mix by the induction hypothesis.
               wefwaefwefwefaweewojweoifaewjfoi

            2. \(A\) is \(\exists xF(x)\). The last part of \(P\) looks like this
               \begin{equation*}
               \begin{prooftree}%[center=false]
               \hypo{\Gamma\to\Delta}
               \hypo{F(a),\Pi_1\to\Lambda}
               \infer1{\exists xF(x),\Pi_1\to\Lambda}
               \infer2[\(\exists xF(x)\)]{\Gamma,\Pi_1^*\to\Delta^*,\Lambda}
               \end{prooftree}
               \end{equation*}
               Let \(b\) be a free variable not occurring in \(P\). Then the result of
               replacing \(a\) by \(b\) throughout the proof ending with \(F(a),\Pi_1\to\Lambda\)
               without a mix, ending with \(F(b),\Pi_1\to\Lambda\), since by the eigenvariable
               condition, \(a\) does not occur in \(\Pi_1\) or \(\Lambda\) (Lemma ref:lemma2.11)

               Consider the following proof:
               \begin{equation*}
               \begin{prooftree}%[center=false]
               \hypo{\Gamma\to\Delta}
               \hypo{F(b),\Pi_1\to\Lambda}
               \infer2[\(\exists xF(x))\)]{\Gamma,F(b),\Pi^*_1\to\Delta^*,\Lambda}
               \end{prooftree}
               \end{equation*}
               By the induction hypothesis, the end-sequent of this proof can be proved without a
               mix (say by \(P')\)). Now consider the proof
               \begin{equation*} 
               \begin{prooftree}%[center=false]
               \hypo{\Gamma\to\Delta}
               \hypo{}
               \ellipsis{\(P'\)}{\Gamma,F(b),\Pi_1^*\to\Delta^*,\Lambda}
               \infer[double]1{F(b),\Gamma,\Pi_1^*\to\Delta^*,\Lambda}
               \infer1{\exists xF(x),\Gamma,\Pi_1^*\to\Delta^*,\Lambda}
               \infer2{\Gamma,\Gamma,\Pi_1^*\to\Delta^*,\Delta^*,\Lambda}
               \end{prooftree}
               \end{equation*}


      2. \(\rank_r(P)=1)\).

   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   The cut-elimination theorem holds for \(\LJ\)
   #+END_theorem


** Some consequences of the cut-elimination theorem
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   By a *subformula* of a formula \(A\) we mean a formula used in building up
   \(A\).

   Two formulas \(A\) and \(B\) are said to be *equivalent*
   in \(\LK\)if \(A\equiv B\) is provable in \(\LK\)

   In a formula \(A\) an occurrence of a logical symbol, say \(\sharp\) is *in the
   scope* of an occurrences of a logical symbol, say \(\natural\), if in the
   construction of \(A\) (from atomic formulas) the stage where \(\sharp\) is
   the outermost logical symbol precedes the stage where \(\natural\) is the
   outermost logical symbol. Further, a symbol \(\sharp\) is said to be in the
   left scope of a \(\supset\) if \(\supset\) occurs in the form \(B\supset C\)
   and \(\sharp\) occurs in \(B\)

   A formula is called *prenex* (in prenex form) if no quantifier in it is in the
   scope of a propositional connective.
   #+END_definition

   A proof without a cut contains only subformulas of the formulas occurring in
   the end-sequent. A formula is provable iff it is provable by use of its
   subformulas only

   #+ATTR_LATEX: :options [consistency]
   #+BEGIN_theorem
   \(\LK\) and \(\LJ\) are consistent
   #+END_theorem

   #+BEGIN_proof
   Suppose \(\to\) were provable in \(\LK\). Then by the cut-elimination
   theorem, it would be provable in \(\LK\) without a cut. But this is
   impossible, by the subformula property of cut-free proofs
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   label:thm6.3
   In a cut-free proof in \(\LK\) (or \(\LJ\)) all the formulas which occur in
   it are subformulas of the formulas in the end-sequent
   #+END_theorem

   #+ATTR_LATEX: :options [Gentzen's midsequent theorem for $\LK$]
   #+BEGIN_theorem
   Let \(S\) be a sequent which consists of prenex formulas only and is provable
   in \(\LK\). Then there is a cut-free proof of \(S\) which contains a sequent
   (called a *midsequent*), say \(S'\), which satisfies the following
   1. \(S'\) is quantifier-free
   2. Every inference above \(S'\) is either structural or propositional
   3. Every inference below \(S'\) is either structural or a quantifier
      inference


   Thus a midsequent splits the proof into an upper part, which contains the
   propositional inferences, and a lower part, which contains the quantifier
   inferences.

   The above holds reading "\(\LJ\) without \(\vee\)left" in place of \(\LK\)
   #+END_theorem
   #+ATTR_LATEX: :options [outline]
   #+BEGIN_proof 

   Combining Proposition ref:prop2.14 and the cut-elimination theorem we may assume that there is a
   cut-free proof of \(S\), say \(P\), in which all the initial sequents consist of atmoic formulas
   only (_why do we need atomic formula_). Let \(I\) be a quantifier inference in \(P\). The number of propositional inference
   under \(I\) is called the order of \(I\). The sum of orders for all the quantifier inferences
   in \(P\)is called the order of \(P\). The proof is carried out by induction on the order
   of \(P\).

   Case 1: The order of a proof \(P\) is 0. If there is a propositional inference, take the
   lowermost such, and call its lower sequent \(S_0\). Above this sequent there is no quantifier
   inference. Therefore if there is a quantifier in or above \(S_0\), then it is introduced by
   weakening. Since the proof is cut-free, the weakening formula is a subformula of one of the
   formulas in the end-sequent. Hence no propositional inferences apply to it. (Since its in prenex form!)
   We can thus eliminate
   these weakenings and obtain a sequent \(S_0'\) corresponding to \(S_0\). By adding some
   weakenings under \(S_0'\) we derive \(S\) and \(S_0'\) serves as the mid-sequent

   If there is no propositional inference in \(P\), then take the uppermost quantifier inferences.
   Its upper sequent serves as a midsequent

   Case 2: The order of \(P\) is not 0. Then there is at least one propositional inference which is
   below a quantifier property. Moreover, there is a quantifier inference \(I\) with the following
   property: the uppermost logical inference under \(I\) is a propositional inference. Call
   it \(I'\). We can lower the order by interchanging the positions of \(I\) and \(I'\). Say \(I\)
   is \(\forall\)right, then proof \(P\) is 
   \begin{equation*}
   \begin{prooftree}%[center=false]
   \hypo{}
   \ellipsis{}{\Gamma\to\Theta,F(a)}
   \infer1[\(I\)]{\Gamma\to\Theta,\forall xF(x)}
   \ellipsis{(*)}{}
   \infer1[\(I'\)]{\Delta\to\Lambda}
   \end{prooftree}
   \end{equation*}
   where the (*)-part of \(P\) contains only structural inferences and \Lambda contains \(\forall xF(x)\) as a
   sequent-formula. Transform \(P\) into the following proof \(P'\):
   \begin{equation*}
   \begin{prooftree}%[center=false]
   \hypo{\Gamma\to\Theta,F(a)}
   \ellipsis{structural inferences}{\Gamma\to F(a),\Theta,\forall xF(x)}
   \ellipsis{}{}
   \infer1[\(I'\)]{\Delta\to F(a),\Lambda}
   \infer[double]1[\(I\)]{\Delta,\Lambda,\forall xF(x)}
   \infer[double]1{\Delta\to\Lambda}
   \ellipsis{}{}
   \end{prooftree}
   \end{equation*}
   It is obvious that the order of \(P'\) is less than that of \(P\)
   #+END_proof
   For technical reasons we introduce the predicate symbol \(\top\) with 0 argument places, and
   admit \(\to\top\) as an additional initial sequent. The system which is obtained from \(\LK\)
   thus extended is denoted by \(\LKsh\)

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   label:lemma6.5
   Let \(\Gamma\to\Delta\) be \(\LK\)-provable, and let \((\Gamma_1,\Gamma_2)\)
   and \(\Delta_1,\Delta_2\) be arbitrary partitions of \Gamma and \Delta, respectively (including the cases
   that one or more of \(\Gamma_1,\Gamma_2,\Delta_1,\Delta_2\) are empty). We denote such a
   partition by \([\{\Gamma_1;\Delta_1\},\{\Gamma_2;\Delta_2\}]\) and call it a partition of the
   sequent \(\Gamma\to\Delta\). Then there exists a formula \(C\) of \(\LKsh\) (called an
   *interpolant* of \([\{\Gamma_1;\Delta_1\},\{\Gamma_2;\Delta_2\}]\)) s.t.
   1. \(\Gamma_1\to\Delta_1,C\) and \(C,\Gamma_2\to\Delta_2\) are both \(\LKsh\)-provable
   2. All free variables and individual and predicate constants in \(C\) (apart from \(\top\)) occur
      both in \(\Gamma_1\cup\Delta_1\) and \(\Gamma_2\cup\Delta_2\)
   #+END_lemma

   #+ATTR_LATEX: :options [Craig's interpolation theorem for \(\LK\)]
   #+BEGIN_theorem
   label:thm6.6
   1. Let \(A\) and \(B\) be two formulas s.t. \(A\supset B\) is \(\LK\)-provable. If \(A\)
      and \(B\) have at least one predicate constant in common, then there exists a formula \(C\),
      called an interpolant of \(A\supset B\) s.t. \(C\) contains only those individual constants,
      predicate constants and free variables that occur in both \(A\) and \(B\) and s.t.
      \(A\supset C\) and \(C\supset B\) are \(\LK\)-provable. If \(A\) and \(B\) contain no
      predicate constant in common, then either \(A\to\) or \(\to B\) is \(\LK\)-provable
   2. As above, with \(\LJ\) inplace of \(\LK\)
   #+END_theorem

   #+BEGIN_proof
   Assume that \(A\supset B\), and hence \(A\to B\) is provable, and \(A\) and \(B\) have at least
   one predicate constant in common. Then by Lemma ref:lemma6.5, taking \(A\) as \(\Gamma_1\)
   and \(B\) as \(\Delta_2\) (with \(\Gamma_2\) and \(\Delta_1\) empty), there exists a
   formula \(C\)satisfying 1 and 2. So \(A\to C\) and \(C\to B\) are \(\LKsh\)-provable. Let \(R\)
   be predicate constant which is common to \(A\) and \(B\) and has \(k\) argument places.
   Let \(R'\) be \(\forall y_1\dots\forall y_kR(y_1,\dots,y_k)\), where \(y_1,\dots,y_k\) are new bound
   variables.  By replacing \(\top\) by \(R'\supset R'\) we can transform \(C\) into a
   formula \(C'\) of the original language, s.t. \(A\to C'\) and \(C'\to B\)
   are \(\LK\)-provable. \(C'\) is then the desired interpolant.

   If there is no predicate common to \(\Gamma_1\cup\Delta_1\) and \(\Gamma_2\cup\Delta_2\) in the
   partition, then by Lemma ref:lemma6.5 there is a \(C\) s.t. \(\Gamma_1\to\Delta_1,C\)
   and \(C,\Gamma_2\to\Delta_2\) are provable, and \(C\) consists of \(\top\) and logical symbols
   only. Then it can easily be shown, by induction on the complexity of \(C\), that either \(\to C\)
   or \(C\to\) is provable. Hence either \(\Gamma_1\to\Delta_1\) or \(\Gamma_2\to\Delta_2\) is provable.
   #+END_proof

   #+ATTR_LATEX: :options [Lemma \cite{lemma6.5}]
   #+BEGIN_proof
   The lemma is proved by induction on the number of inferences \(k\), in a cut-free proof
   of \(\Gamma\to\Delta\). At each stage there are several cases to consider; we deal with some
   examples only.
   1. \(k=0,\Gamma\to\Delta\)  has the form \(D\to D\). There are four
      cases: 1.
      \([\{D;D\},\{;\}]\), 2. \([\{;\},\{D;D\}]\), 3. \([\{D;\},\{;D\}]\), 4. \(\{;D\},\{D;\}\).
      Take for \(C:\neg\top\) in 1, \(\top\) in 2, \(D\) in 3 and \(\neg D\) in 4
   2. \(k>0\) and the last inference is \(\wedge\)right:
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{\Gamma\to\Delta,A}
      \hypo{\Gamma\to\Delta,B}
      \infer2{\Gamma\to\Delta,A\wedge B}
      \end{prooftree}
      \end{equation*}
      Suppose the partition is \([\{\Gamma_1;\Delta_1,A\wedge B\},\{\Gamma_2;\Delta_2\}]\). Consider
      the induced partition of the upper sequents,
      viz \([\{\Gamma_1;\Delta_1,A\},\{\Gamma_2;\Delta_2\}]\)
      and \([\{\Gamma_1;\Delta_1,B\},\{\Gamma_2;\Delta_2\}]\) respectively. By the induction
      hypothesis applied to the subproofs of the upper sequents, there exists interpolants \(C_1\)
      and \(C_2\) so that
      \(\Gamma_1\to\Delta_1,A,C_1\);\(C_1,\Gamma_2\to\Delta_2\);\(\Gamma_1\to\Delta_1,B,C_2\)
      and \(C_2,\Gamma_2\to\Delta_2\) are all \(\LKsh\)-provable. From these
      sequents, \(\Gamma_1\to\Delta_1,A\wedge B,C_1\vee C_2\) and \(C_1\vee C_2,\Gamma_2\to\Delta_2\)
   3. \(k>0\) and the last inference is \(\forall\)left
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{F(s),\Gamma\to\Delta}
      \infer1{\forall xF(x),\Gamma\to\Delta}
      \end{prooftree}
      \end{equation*}
      Suppose \(b_1,\dots,b_n\) are all the free variables and constants which occur in \(s\).
      Suppose the partition is \([\{\forall xF(x),\Gamma_1;\Delta_1\},\{\Gamma_2;\Delta_2\}]\). Consider
      the induced partition of the upper sequent and apply the induction hypothesis. So there exists
      and interpolant \(C(b_1,\dots,b_n)\) so that 
      \begin{align*}
      &F(s),\Gamma_1\to\Delta_1,C(b_1,\dots,b_n)\\
      &C(b_1,\dots,b_n),\Gamma_2\to\Delta_2
      \end{align*}
      are \(\LKsh\)-provable. Let \(b_{i_1},\dots,b_{i_m}\) be all the variables and constants
      among \(b_1,\dots,b_n\) which do not occur in \(\{F(x),\Gamma_1;\Delta_1\}\) . Then
      \begin{equation*}
      \forall y_1\dots\forall y_mC(b_1,\dots,y_1,\dots,y_m,\dots,b_n)
      \end{equation*}
      where \(b_{i_1},\dots,b_{i_m}\) are replaced by the bound variables, serve as the required
      interpolant.
      
   4. \(k>0\) and the last inference is \(\forall\)right
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{\Gamma\to\Delta,F(a)}
      \infer1{\Gamma\to\Delta,\forall xF(x)}
      \end{prooftree}
      \end{equation*}
      where \(a\) doesn't occur in the lower sequent.

      Suppose the partition is \([\{\Gamma_1;\Delta_1,\forall xF(x)\},\{\Gamma_2;\Delta_2\}]\). By the
      induction hypothesis there exists an interpolant \(C\) so that \(\Gamma_1\to\Delta_1,F(a),C\)
      and \(C,\Gamma_2\to\Delta_2\) are provable. Since \(C\) doesn't contain \(a\), we can derive
      \begin{equation*}
      \Gamma_1\to\Delta_1,\forall xF(x),C
      \end{equation*}
      and hence \(C\) serves as the interpolant
   #+END_proof

   #+BEGIN_exercise
   label:ex6.7
   Let \(A\) and  \(B\) be prenex formulas which have only \(\forall\) and \(\wedge\) as logical
   symbols. Assume futhermore that there is at least one predicate constant common to \(A\)
   and \(B\). Suppose \(A\supset B\) is provable.

   Show that there exists a formula \(C\) s.t.
   1. \(A\supset C\) and \(C\supset B\) are provable
   2. \(C\) is a prenex formula
   3. the only logical symbols in \(C\) are \(\forall \) and \(\wedge\)
   4. the predicate constants in \(C\) are common to \(A\) and \(B\)
   #+END_exercise

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   1. A *semi-term* is an expression like a term, except that bound variables are allowed in its
      construction. Let \(t\) be a term and \(s\) a semi-term. We call \(s\) a *sub-semi-term*
      of \(t\) if
      1. \(s\) contain a bound variable (\(s\) is not a term)
      2. \(s\) is not a bound variable itself
      3. some subterm of \(t\) is obtained from \(s\) by replacing all the bound variables in \(s\)
         by appropriate terms
   2. A *semi-formula* is an expression like a formula, except that bound variables are (also) allowed
      to occur free in it
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   Let \(t\) be a term and \(S\) a provable sequent satisfying
   \begin{equation}
   \label{eq:1}
   \text{There is no sub-semi-term of }t\text{ in }S
   \end{equation}
   Then the sequent which is obtained from \(S\) by replacing all the occurrences of \(t\) in \(S\)
   by a free variable is also provable
   #+END_theorem

   #+BEGIN_proof
   Consider a cut-free regular proof of \(S\), say \(P\).  If ref:eq:1 holds for the lower sequent
   of an inference in \(P\) then it holds for the upper sequents. The theorem follows by
   mathematical induction on the number of inferences in \(P\)
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Let \(R_1,\dots,R_m,R\) be predicate constants. Let \(A(R,R_1,\dots,R_m)\) be a sentence in which
   all occurrences of \(R,R_1,\dots,R_m\) are indicated. Let \(R'\) be a predicate constant with the
   same number of argument-places as \(R\). Let \(B\)
   be \(\forall x_1\dots\forall x_k(R(x_1,\dots,x_k)\equiv R'(x_1,\dots,x_k))\), where the string of
   quantifiers is empty if \(k=0\). Let \(C\) be \(A(R,R_1,\dots,R_m)\wedge A(R',R_1,\dots,R_m)\). We say
   that \(A(R,R_1,\dots,R_m)\) *defines (in \(\LK\)) \(R\) implicitly* in terms of \(R_1,\dots,R_m\)
   if \(C\supset B\) is (\(\LK\)-)provable and we say that \(A(R,R_1,\dots,R_m)\) *defines
   (in \(\LK\)) \(R\) explicitly* in terms of \(R_1,\dots,R_m\) and the individual constants
   in \(A(R,R_1,\dots,R_m)\) if there exists a formula \(F(a_1,\dots,a_k)\) containing only the
   predicate constants \(R_1,\dots,R_m\) and the individual constants in \(A(R,R_1,\dots,R_m)\) s.t.
   \begin{equation*}
   A(R,R_1,\dots,R_m)\to\forall x_1\dots\forall x_k
   (R(x_1,\dots,x_k)\equiv F(x_1,\dots,x_k))
   \end{equation*}
   is \(\LK\)-provable
   #+END_definition

   #+ATTR_LATEX: :options [Beth's definability theorem for $\LK$]
   #+BEGIN_proposition
   If a predicate constant \(R\) is defined implicitly in terms of \(R_1,\dots,R_m\)
   by \(A(R,R_1,\dots,R_m)\), then \(R\) can be defined explicitly in terms of \(R_1,\dots,R_m\) and
   the individual constants in \(A(R,R_1,\dots,R_m)\)
   #+END_proposition

   #+ATTR_LATEX: :options [outline]
   #+BEGIN_proof
   Let \(c_1,\dots,c_n\) be free variables not occurring in \(A\). Then
   \begin{equation*}
   A(R,R_1,\dots,R_m),A(R',R_1,\dots,R_m)\to
   R(c_1,\dots,c_n)\equiv R'(c_1,\dots,c_n)
   \end{equation*}
   and hence also
   \begin{equation*}
   A(R,R_1,\dots,R_m)\wedge R(c_1,\dots,c_k)\to A(R',R_1,\dots,R_m)\supset R'(c_1,\dots,c_n)
   \end{equation*}
   are provable. Now apply Craig's theorem to the latter sequent. We get
   \begin{align*}
   &A(R,R_1,\dots,R_m)\wedge R(c_1,\dots,c_k)\supset F(c_1,\dots,c_k)\\
   &F(c_1,\dots,sc_k)\supset A(R',R_1,\dots,R_m)\supset R'(c_1,\dots)
   \end{align*}
   First line implies \(A(R,R_1,\dots,R_m)\to R(c_1,\dots,c_k)\supset F(c_1,\dots,c_k)\). The second line
   with the assumption \(A(R,R_1,\dots,R_m)\) shows that
   \(A(R,R_1,\dots,R_m)\to F(c_1,\dots,c_k)\supset R(c_1,\dots,c_k)\)
   #+END_proof

   #+ATTR_LATEX: :options [Robinson]
   #+BEGIN_proposition
   Assume that the language contains no function constants. Let \(\cala_1\) and \(\cala_2\) be two
   consistent axiom systems. Suppose furthermore that, for any sentence \(A\) which is dependent
   on \(\cala_1\) and \(\cala_2\), it is not the case that \(\cala_1\to A\) and \(\cala_2\to\neg A\)
   are provable. Then \(\cala_1\cup\cala_2\) is consistent
   #+END_proposition

   #+BEGIN_proof
   Suppose \(\cala_1\cup\cala_2\) is not consistent. Then there are axiom sentences \(\Gamma_1\)
   and \(\Gamma_2\) from \(\cala_1\) and \(\cala_2\) respectively s.t. \(\Gamma_1,\Gamma_2\to\) is
   provable. Since \(\cala_1\) and \(\cala_2\) are each consistent, neither \(\Gamma_1\)
   nor \(\Gamma_2\) is empty. Apply Lemma ref:lemma6.5 to the partition \([\{\Gamma_1;\},\{\Gamma_2;\}]\)
   #+END_proof

   Let \(\LKp\) and \(\LJp\) denote the quantifier-free parts of \(\LK\) and \(\LJ\)

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   There exist decision procedures for \(\LKp\) and \(\LJp\)
   #+END_theorem

   #+BEGIN_proof
   The following decision procedure was given by gentzen. A sequent of \(\LKp\)  (or \(\LJp\)) is
   said to be *reduced* if in the antecedent the same formula does not occur at more than three places
   as sequent formulas, and likewise in the succedent. A sequent \(S'\) is called a *reduct* of a
   sequent \(S\) is \(S'\) is reduced and is obtained from \(S\) by deleting some occurrences of
   formulas. Now given a sequent \(S\) of \(\LKp\) (or \(\LJp\)), let \(S'\) be any reduct of \(S\).
   We note the following
   1. \(S\) is provable or unprovable according as \(S'\) is provable or unprovable
   2. The number of all reduced sequents which contain only subformulas of the formula in \(S\) is
      finite


   Consider the finite system of sequents as in 2, say \(\cali\).  Collect all initial sequents in
   the systems. Call this set \(\cali_0\). Then examine \(\cali-\cali_0\) to see if there is a
   sequent which can be the lower sequent of an inference whose upper sequent(s) is (are) one (two)
   sequent(s) from \(\cali_0\). Call the set of all sequents which satisfy this
   condition \(\cali_1\). Now see if there is a sequent in \((\cali-\cali_0)-\cali_1\) which be the
   lower sequent of an inference whose upper sequent(s) is (are) one (two) of the sequent(s)
   in \(\cali_0\cup\cali_1\). Continue this process until either the sequent \(S'\) itself is
   determined as provable, or the process does not give any new sequent as provable. One of the two
   must happen. (Note that the whole argument is finitary)
   #+END_proof

   #+ATTR_LATEX: :options [Harrop]
   #+BEGIN_theorem
   <<Problem2>>
   1. Let \Gamma be a finite sequence of formulas s.t. in each formula of \Gamma every occurrence
   of \(\vee\) and \(\exists\) is either in the scope of a \(\neg\) or in the left scope of
   a \(\sup\). This condition will be referred to as (*) in this theorem.
      1. Then \(\Gamma\to A\vee B\) is \(\LJ\)-provable iff  \(\Gamma\to A\) and \(\Gamma\to B\) is \(\LJ\)-provable
      2. \(\Gamma\to\exists xF(x)\) is \(\LJ\)-provable iff for some term \(s\), \(\Gamma\to F(s)\) is \(\LJ\)-provable
   3. The following sequents (which are \(\LK\)-provable) are not \(\LJ\)-provable
      \begin{gather*}
      \neg(\neg A\wedge\neg B)\to A\vee B;\hspace{1cm}
      \neg\forall x\neg F(x)\to\exists xF(x)\\
      A\supset B\to A\vee B;\hspace{1cm}
      \neg\forall xF(x)\to\exists x\neg F(x);\\
      \neg A(\wedge B)\to A\vee \neg B
      \end{gather*}
   #+END_theorem

   #+BEGIN_proof
   1. 
      1. \(\Rightarrow\). Consider a cut-free proof of \(\Gamma\to A\vee B\). The proof is carried
         out by induction on the number of inferences below all the inferences for \(\vee\)
         and \(\exists\) in the given proof. If the last inference is \(\vee\)right, there is
         nothing to prove. Notice that the last inference cannot be \(\vee,\neg\) or \(\exists\)left

         Case 1: The last inference is \(\wedge\)left
         \begin{equation*}
         \begin{prooftree}%[center=false]
         \hypo{C,\Gamma\to A\vee B}
         \infer1{C\wedge D,\Gamma\to A\vee B}
         \end{prooftree}
         \end{equation*}
         Its obvious that \(C\) satisfies the condition (*). Thus the induction hypothesis applies
         to the upper sequent; hence either \(C,\Gamma\to A\) or \(C,\Gamma\to B\) is provable. In
         either case, the end-sequent can be derived in \(\LJ\)
         Case 2: The last inference is \(\supset\)left
         \begin{equation*}
         \begin{prooftree}%[center=false]
         \hypo{\Gamma\to C}
         \hypo{D,\Gamma\to A\vee B}
         \infer2{C\supset D,\Gamma\to A\vee B}
         \end{prooftree}
         \end{equation*}
         \(D\) satisfies the condition; thus by the induction hypothesis applied to the right upper
         sequent, \(D,\Gamma\to A\) or \(D,\Gamma\to B\) is provable.
      2. If \(\Gamma\to F(s)\) is \(\LJ\)-provable for some term \(s\).
   #+END_proof

** The predicate calculus with equality                               :problem:
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   label:def7.1
   The predicate calculus with equality (denoted \(\LKe\)) can be obtained from \(\LK\) by
   specifying constant of two argument (=: read equals) and adding the following sequents as
   additional initial sequents (\(a=b\) denoting \(=(a,b)\))
   \begin{gather*}
   \to s=s\\
   s_1=t_1,\dots,s_n=t_n\to f(s_1,\dots,s_n)=f(t_1,\dots,t_n)
   \end{gather*} 
   for every function constant \(f\) of \(n\) argument-places (\(n=1,2,\dots\)):
   \begin{equation*}
   s_1=t_1,\dots,s_n=t_n,R(s_1,\dots,s_n)\to R(t_1,\dots,t_n)
   \end{equation*}
   for every predicate constant \(R\) of \(n\) argument; where \(s,s_1,\dots,s_n,t_1,\dots,t_n\) are
   arbitrary terms

   Each such sequent may be called an equality axiom of \(\LKe\)
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop7.2
   Let \(A(a_1,\dots,a_n)\) be an arbitrary formula. Then
   \begin{equation*}
   s_1=t_1,\dots,s_n=t_n,A(s_1,\dots,s_n)\to A(t_1,\dots,t_n)
   \end{equation*}
   is provable in \(\LKe\) for any terms \(s_i,t_i\). Furthermore, \(s=t\to t=s\)
   and \(s_1=s_2,s_2=s_3\to s_1=s_3\) are also provable
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   label:def7.3
   Let \(\Gamma_e\) be the set (axiom system) consisting of the following sentences
   \begin{gather*}
   \forall x(x=x)\\ 
   \forall x_1\dots\forall x_n\forall y_1\dots\forall y_n[x_1=y_1\wedge\dots\wedge
   x_n=y_n\supset f(x_1,\dots,x_n=f(y_1,\dots,y_n))]
   \end{gather*}
   for every function constant \(f\) with \(n\) arguments,
   \begin{equation*}
   \forall x_1\dots\forall x_n\forall y_1\dots\forall y_n[x_1=y_1\wedge\dots\wedge
   x_n=y_n\supset R(x_1,\dots,x_n=R(y_1,\dots,y_n))]
   \end{equation*}
   for every predicate constant \(R\) of \(n\) arguments. Each such sentence is called an *equality axiom*
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   A sequent \(\Gamma\to\Delta\) is provable in \(\LKe\) iff \(\Gamma,\Gamma_e\to\Delta\) is provable in \(\LK\)
   #+END_proposition

   #+BEGIN_proof
   All the initial sequents of \(\LKe\) are provable from \(\Gamma_e\)
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   label:def7.5
   If the cut formula of a cut in \(\LKe\) is of the form \(s=t\), then the cut is called
   *inessential*. It's called *essential* otherwise
   #+END_definition

   #+ATTR_LATEX: :options [the cut-elimination theorem for $\LKe$]
   #+BEGIN_theorem
   If a sequent of \(\LKe\) is \(\LKe\)-provable, then it is \(\LKe\)-provable without an essential cut
   #+END_theorem

   #+BEGIN_proof
   The theorem is proved by removing essential cuts (mixes as a matter of a fact), following the
   method used for Theorem ref:thm5.1

   If the rank is 2, \(S_2\) is an equality axiom and the mix formula is not of the form \(s=t\),
   then the mix formula is of the form \(P(t_1,\dots,t_n)\). If \(S_1\) is also an equality axiom,
   then it has the form
   \begin{equation*}
   s_1=t_1,\dots,s_n=t_n,P(s_1,\dots,s_n)\to P(t_1,\dots,t_n)
   \end{equation*}
   From this and \(S_2\), i.e.,
   \begin{equation*}
   t_1=r_1,\dots,t_n=r_n,P(t_1,\dots,t_n)\to P(r_1,\dots,r_n)
   \end{equation*}
   we obtain by a mix
   \begin{equation*}
   s_1=t_1,\dots,s_n=t_n,t_1=r_1,\dots,t_n=r_n,P(s_1,\dots,s_n)\to P(r_1,\dots,r_n)
   \end{equation*}
   This may be replaced by
   \begin{align*}
   &s_i=t_i,t_i=r_i\to s_i=r_i\quad(i=1,2,\dots,n)\\
   &s_1=r_1,\dots,s_n=r_n,P(s_1,\dots,s_n)\to P(r_1,\dots,r_n)
   \end{align*}
   and then repeated cuts of \(s_i=r_i\) to produce the same end-sequent. All cuts introduced here
   are inessential

   If \(P(t_1,\dots,t_n)\) in \(S_2\) is a weakening formula, then the mix inference is
   \begin{equation*}
   \begin{prooftree}%[center=false]
   \hypo{s_1=t_1,\dots,s_n=t_n,P(s_1,\dots,s_n)\to P(t_1,\dots,t_n)}
   \hypo{P(t_1,\dots,t_n),\Pi\to\Lambda}
   \infer2{s_1=t_1,\dots,s_n=t_n,P(s_1,\dots,s_n),\Pi\to\Lambda}
   \end{prooftree}
   \end{equation*}
   Transform this into
   \begin{equation*}
   \begin{prooftree}%[center=false]
   \hypo{\Pi\to\Lambda}
   \infer[double]1{\text{end-sequent}}
   \end{prooftree}
   \end{equation*}
   #+END_proof

   #+BEGIN_exercise
   label:ex7.7
   A sequent of the form
   \begin{equation*}
   s_1=t_1,\dots,s_n=t_n\to s=t
   \end{equation*}
   is said to be simple if it is obtained from sequents of the following four forms by applications
   of exchanges, contractions, cuts, and weakening left.
   1. \(\to s=s\)
   2. \(s=t\to t=s\)
   3. \(s_1=s_2,s_2=s_3\to s_1=s_3\)
   4. \(s_1=t_1,\dots,s_m=t_m\to f(s_1,\dots,s_m)=f(t_1,\dots,t_m)\)


   Prove that if \(s_1=s_1,\dots,s_m=s_m\to s=t\) is simple, then \(s=t\) is of the form \(s=s\). As
   a special case, if \(\to s=t\) is simple, then \(s=t\) is of the form \(s=s\)

   Let \(\LKe'\) be the system which is obtained from \(\LK\) adding the following sequents as
   initial sequents
   1. simple sequents
   2. sequents of the form
      \begin{equation*}
      s_1=t_1,\dots,s_m=t_m,R(s_1',\dots,s_n')\to R(t_1',\dots,t_n')
      \end{equation*}
      where \(s_1=t_1,\dots,s_m=t_m\to s_i'=t_i'\) is simple for each \(i\)


   First prove that the initial sequents of \(\LKe'\) are closed under cuts and that if
   \begin{equation*}
   R(s_1,\dots,s_n)\to R(t_1,\dots,t_n)
   \end{equation*}
   is an initial sequent of \(\LKe'\) (where \(R\) is not =), then it is of the form \(D\to D\).
   Finally prove that the cut-elimination theorem (without the exception of inessential cuts)
   holds for \(\LKe'\)
   #+END_exercise

   #+BEGIN_proof
   1. Consider the complexity of \(s\)?

      If \(s\) is a variable, we can only get this by \(v_i=v_i\)
   #+END_proof

** The completeness theorem
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   1. Let \(L\) be a language. By a *structure* for \(L\) we mean a pair \(\la D,\phi\ra\), where \(D\)
      is a non-empty set and \phi is a map from the constants of \(L\) s.t.
      1. if \(k\) is an individual constant, then \(\phi k\) is an element of \(D\)
      2. if \(f\) is a function constant of \(n\) arguments, then \(\phi f\) is a mapping from \(D^n\) to \(D\)
      3. if \(R\) is a predicate constant of \(n\) arguments, then \(\phi R\) is a subset of \(D^n\)
   2. An *interpretation* of \(L\) is a structure \(\la D,\phi\ra\) together with a mapping \(\phi_0\)
      from variables into \(D\). We may denote an interpretation \((\la D,\phi\ra,\phi_0)\) simply
      by \(\fF\). \(\phi_0\) is called an assignment from \(D\)
   3. We say that an interpretation \(\fF=(\la C,\phi\ra,\phi_0)\) *satisfies* a formula \(A\) if this
      follows from the following inductive definition
      1. For every semi-term \(t\), \(\phi(a)=\phi_0(a)\) and  for a\(\phi(x)=\phi_0(x)\)ll free
         variables \(a\) and bound variables \(x\). next if \(f\) is a function constant and \(t\)
         is a semi-term for which \(\phi t\) is already defined, then \(\phi(f(t))\) is defined to
         be \((\phi f)(\phi t)\)
   #+END_definition

   #+ATTR_LATEX: :options [Completeness and soundness]
   #+BEGIN_theorem
   A formula is provable in \(\LK\) iff it is valid
   #+END_theorem

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   label:lemma8.3
   Let \(S\) be a sequent. Then either there is a cut-free proof of \(S\), or there is an
   interpretation which does not satisfy \(S\) (and hence \(S\) is not valid)
   #+END_lemma

   #+BEGIN_proof
   We will define, for each sequent \(S\), a (possibly infinite) tree, called the reduction tree
   for \(S\), from which we can obtain either a cut-free proof of \(S\) or an interpretation not
   satisfying \(S\). This reduction tree for \(S\) contains a sequent at each node. It is
   constructed in stages as follows

   Stage 0: Write \(S\) at the bottom of the tree

   Stage \(k\) (\(k>0\)): This is defined by cases
   1. Every topmost sequent has a formula common to its antecedent and succedent. Then stop.
   2. This stage is defined according as
      \begin{equation*}
      k\equiv 0,1,2,\dots,12\mod 13
      \end{equation*}
      \(k\equiv0\) and \(k\equiv1\) concern the symbol \(\neg\); \(k\equiv2\) and \(k\equiv3\)
      concern \(\wedge\); \(k\equiv4\) and \(k\equiv5\) concern \(\vee\); \(k\equiv6\)
      and \(k\equiv7\) concern \(\supset\); \(k\equiv8\) and \(k\equiv9\)
      concern \(\forall\); \(k\equiv10\) and \(k\equiv11\) concern equiv \(\exists\)


   Assume that there are no individual or function constants

   All the free variables which occur in any sequent which has been obtained at or before
   stage \(k\) are said to be "available at stage \(k\)". In case there is none, pick any free
   variable and say that it is available
   0. [@0] \(k\equiv 0\). Let \(\Pi\to\Lambda\) be any topmost sequent of the tree which has been defined
      by stage \(k-1\). Let \(\neg A_1,\dots,\neg A_n\) be all the formulas in \Pi whose outermost logical
      symbol is \(\neg\), and to which no reduction has been applied in previous stages. Then write
      down
      \begin{equation*}
      \Pi\to\Lambda,A_1,\dots,A_n
      \end{equation*}
      above \(\Pi\to\Lambda\). We say that a \(\neg\)left reduction has been applied
      to \(\neg A_1,\dots,\neg A_n\)
   1. \(k\equiv1\). Let \(\neg A_1,\dots,\neg A_n\) be all the formulas in \Lambda whose outermost logical
      symbol is \(\neg\) and to which no reduction has been applied so far. Then write down
      \begin{equation*}
      A_1,\dots,A_n,\Pi\to\Lambda
      \end{equation*}
      above \(\Pi\to\Lambda\). We say that a \(\neg\)right reduction has been applied
      to \(\neg A_1,\dots,\neg A_n\).
   2. \(k\equiv2\). Let \(A_1\wedge B_1,\dots,A_n\wedge B_n\) be all the formulas in \Pi whose
      outermost logical symbols is \(\wedge\) and to which no reduction has been applied yet. Then
      write down
      \begin{equation*}
      A_1,B_1,A_2,B_2,\dots,A_n,B_n,\Pi\to\Lambda
      \end{equation*}
      above \(\Pi\to\Lambda\). We say that an \(\wedge\)left reduction has been applied to
      \begin{equation*}
      A_1\wedge B_1,\dots,A_n\wedge B_n
      \end{equation*}
   3. \(k\equiv3\). Let \(A_1\wedge B_1,\dots,A_n\wedge B_n\) be all the formulas in \Pi whose
      outermost logical symbols is \(\wedge\) and to which no reduction has been applied yet. Then
      write down
      \begin{equation*}
      \Pi\to\Lambda, C_1,\dots,C_n
      \end{equation*}
      where \(C_i\) is either \(A_i\) or \(B_i\), above \(\Pi\to\Lambda\). Take all possible
      combinations of such; so there are \(2^n\) such sequents above \(\Pi\to\Lambda\). We say that
      an \(\wedge\)right reduction has been applied to \(A_1\wedge B_1,\dots,A_n\wedge B_n\)
   4. \(k\equiv 4\). \(\vee\)left, similar to 3
   5. \(k\equiv5\). \(\vee\)right, similar to 2.
   6. \(k\equiv6\). Let \(A_1\supset B_1,\dots,A_n\supset B_n\) be all the formulas in \Pi whose
      outermost symbol is \(\supset\) and to which no reduction has been applied yet. Then write
      down the following sequents above \(\Pi\to\Lambda\)
      \begin{equation*}
      B_{i_1},B_{i_2},\dots,B_{i_k},\Pi\to\Lambda,A_{j_1},\dots,A_{j_{n-k}}
      \end{equation*}
      where \(i_1<i_2<\dots<i_k\), \(j_1<j_2<\dots<j_{n-k}\) and
      \((i_1,\dots,i_k,j_1,\dots,j_{n-k})\) is a permutation of \((1,2,\dots,n)\). Take all possible
      permutations: so there are \(2^n\) such sequents.
   7. \(k\equiv 7\). Let \(A_1\supset B_1, \dots,A_n\supset B_n\) be all the formulas in \Lambda whose outermost logical symbol
      is \(\supset\) and to which no reduction has been applied yet. Then write down
      \begin{equation*}
      A_1,A_2,\dots,A_n,\Pi\to\Lambda,B_1,\dots,B_n
      \end{equation*}
      above \(\Pi\to\Lambda\). We say that an \(\supset\)right reduction has been applied to
      \begin{equation*}
      A_1\supset B_1,\dots,A_n\supset B_n
      \end{equation*}
   8. \(k\equiv 8\). Let \(\forall x_1A_1(x_1),\forall x_nA_n(x_n)\) be all the formulas in \Pi whose outermost logical
      symbol is \(\forall\). let \(a_i\) be the first variable available at this stage which has not been
      used for reduction of \(\forall x_iA_i(x\) for \(1\le i\le n\). Then write down
      \begin{equation*}
      A_1(a_1),\dots,A_n(a_n),\Pi\to\Lambda
      \end{equation*}
      above \(\Pi\to\Lambda\). We say that a \(\forall\)left reduction has been applied to
      \begin{equation*}
      \forall x_1A_1(x), \dots,\forall x_nA_n(x_n)
      \end{equation*}
   9. \(k\equiv v\). Let \(\forall x_1A_1(x_1),\dots,\forall x_nA_n(x_n)\) be all formulas in \Lambda whose outermost
      logical symbol is \(\forall\) and to which no reduction has been applied so far. Let
      \(a_1,\dots,a_n\) be the first \(n\) free variables which are not available at this stage.
      Then write down
      \begin{equation*}
      \Pi\to\Lambda,A_1(a_1),\dots,A_n(a_n)
      \end{equation*}
      above \(\Pi\to\Lambda\) . We say that a \(\forall\)right reduction has been applied
      to \(\forall x_1A_1(x_1),\dots,\forall x_nA_n(x_n)\). Notice that \(a_1,\dots,a_n\) are new available free variables
   10. \(k\equiv10\). \(\exists\)left reduction. Similar to 9
   11. \(k\equiv 11\). \(\exists\)right reduction. similar to 8
   12. If \Pi and \Lambda have any formula in common, write nothing above \(\Pi\to\Lambda\). If \Pi and
       \Lambda have no formula in common and the reductions described in 0-11 are not applicable, write
       the same sequent \(\Pi\to\Lambda\) again above it.


   So the collection of those sequents which are obtained by the above reduction process, together
   with the partial order obtained by this process, is the reduction tree (for \(S\)). It is denoted
   by \(T(S)\). We will construct "reduction trees" like this again

   As an example of the case where the reduction process does not terminate, consider a sequent of
   the form \(\forall x\exists y A(x,y)\to\), where \(A\) is a predicate

   Now a (finite or infinite) sequence \(S_0,S_1,\dots\) of sequents in \(T(S)\) is called a branch
   if
   1. \(S_0=S\)
   2. \(S_{i+1}\) stands immediately above \(S_i\)
   3. if the sequence is finite, say \(S_1,\dots,S_n\), then \(S_n\) has the form \(\Pi\to\Lambda\),
      where \Pi and \Lambda have a formula in common


   Now given a sequent \(S\), let \(T\) be the reduction tree \(T(S)\). If each branch of \(T\) ends
   with a sequent whose antecedent and succedent contain a formula in common, then it is a routine
   task to write a proof without a cut ending with \(S\) by suitably modifying \(T\). Otherwise
   there is an infinite branch. Consider such a branch consisting of sequents
   \(S=S_0,S_1,\dots,S_n,\dots\)

   Let \(S_i\) be \(\Gamma_i\to\Delta_i\). let \(\bigcup\Gamma\) be the set of all formulas
   occurring in \(\Gamma_i\) for some \(i\), and let \(\bigcup\Delta\) be the set of all formulas
   occurring in \(\Delta_j\) for some \(j\). We shall define an interpretation in which every
   formula in \(\bigcup\Gamma\) holds and no formula in \(\bigcup\Delta\) holds. Thus \(S\) does not
   hold in it.

   First notice that from the way the branch was chosen, \(\bigcup\Gamma\) and \(\bigcup\Delta\)
   have no atomic formula in common. Let \(D\) be the set of all the free variables. We consider the
   interpretation \(\fF=(\la D,\phi\ra,\phi_0)\) where \phi and \(\phi_0\) are defined as follows:
   \(\phi_0(a)=a\) for all free variables \(a\), \(\phi_0(x)\) is defined arbitrarily for all bound
   variables \(x\). For an \(n\)-ary predicate constant \(R\), \(\phi R\) is any subset of \(D^n\)
   s.t.:
   if \(R(a_1,\dots,a_n)\in\bigcup\Gamma\), then \((a_1,\dots,a_n)\in\phi R\), and
   \((a_1,\dots,a_n)\not\in\phi R\)

   We claim that this interpretation \(\fF\) has the required property: it satisfies every formula
   in \(\bigcup\Gamma\), but no formula in \(\bigcup\Delta\). We prove this by induction on the
   number of logical symbols in the formula \(A\). We consider here only the case where \(A\) is of
   the form \(\forall xF(x)\) and assume the induction hypothesis. For the base case, note that \(\bigcup\Gamma\cap\bigcup\Delta=\emptyset\).

   1. \(A\) is in \(\bigcup\Gamma\). Let \(i\) be the least number s.t. \(A\) is in \(\Gamma_i\).
      Then \(A\) is in \(\Gamma_j\) for all \(j>i\). It is sufficient to show that all substitution
      instances \(A(a)\), for \(a\in D\), are satisfied by \(\fF\).
   2. \(A\) is in \(\bigcup\Delta\). Consider the step at which \(A\) was used to define an upper
      sequent from \(\Gamma_i\to\Delta_i\). It looks like
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{\Gamma_{i+1}\to\Delta_{i+1}^1,F(a),\Delta_{i+1}^2}
      \infer1{\Gamma_1\to\Delta_i^1,A,\Delta_1^2}
      \end{prooftree}
      \end{equation*}
      Then by the induction hypothesis, \(F(a)\) is not satisfied by \(\fF\), so \(A\) is not
      satisfied by \(\fF\) either.
   #+END_proof

   #+ATTR_LATEX: :options {Feferman}
   #+BEGIN_exercise 
   label:ex8.4 
   Let \(J\) be a non-empty set. Each element of \(J\) is called a *sort*. A many-sorted language for
   the set of sorts \(J\), say \(L(J)\), consists of the following
   1. Individual constants: \(k_0,k_1,\dots,k_i,\dots\), where to each \(k_i\) is assigned one sort
   2. Predicate constants: \(R_0,R_1,\dots,R_i,\dots\), where to each \(R_i\) is assigned a number
      \(n\ge0\) and sorts \(j_1,\dots,j_n\). We say that \((n;j_1,\dots,j_n)\) is assigned to \(R_i\)
   3. Function constants: \(f_0,\dots,f_i,\dots\) where to each \(f_i\) is assigned a number
      \(n\ge1\) and sorts \(j_1,\dots,j-n,j\). We say that \((n;j_1,\dots,j_n,j)\) is assigned to \(f_i\).
   4. Free variables of sort \(j\) for each \(j\) in \(J\): \(a_0^j,a_1^j,\dots,a_i^j,\dots\)
   5. Bound variables of sort \(j\) for each \(j\) in \(J\)
   6. Logical symbols: \(\neg,\wedge,\vee,\supset,\forall, \exists\)


   Terms of sort \(j\) for each \(j\) are defined as follows. Individual constants and free
   variables of sort \(j\) are terms of sort \(j\); if \(f\) is a function constant with
   \((n;j_1,\dots,j_n,j)\) assigned to it and \(t_1,\dots,t_n\) are terms of sort \(j_1,\dots,j_n\),
   respectively, then \(f(t_1,\dots,t_n)\) is a term of sort \(j\)

   If \(R\) is a predicate constant with \((n;j_1,\dots,j_n)\) assigned to it and \(t_1,\dots,t_n\)
   are terms of sort \(j_1,\dots,j_n\), respectively, then \(R(t_1,\dots,t_n)\) is an atomic
   formula. If \(F(a^j)\) is a formula and \(x^j\) does not occur in \(F(a^j)\) then \(\forall x^jF(x_j)\)
   and \(\exists x^jF(x^j)\) are formulas.

   The rules of inference are those of \(\LK\), except that in the rules for \(\forall\) and
   \(\exists\), terms and free variables must be replaced by bound variables of the same sort

   Prove the following
   1. The cut-elimination theorem holds for the system just defined 

   \(\Sort(A)\) is the set  of \(j\) in \(J\) s.t. a symbol of sort \(j\) occurs
   in \(A\); \(\Ex(A)\) and \(\Un(A)\) are the sets of sorts of bound variables which occur in some
   essentially existential, respectively universal quantifier in \(A\). (An occurrence
   of \(\exists\), say \(\sharp\), is said to be *essentially existential* or *universal* according to
   the following definition. Count the number of \(\neg\) and \(\supset\) in \(A\) s.t. \(\sharp\)
   is either in the scope of \(\neg\), or in the left scope of \(\supset\). If this number is even,
   then \(\sharp\) is essentially existential in \(A\), while if it is odd then \(\sharp\) is
   essentially universal. We define dually for \(\forall\)) . \(\Fr(A)\) is the set of free
   variables in \(A\). \(\Pr(A)\) is the set of predicate constants in \(A\)

   2. [@2] Suppose \(A\supset B\) is provable in the above system and at least one
      of \(\Sort(A)\cap\Ex(B)\) and \(\Sort(B)\cap\Un(A)\) is not empty. Then there is a
      formula \(C\) s.t. \(\sigma(C)\subseteq\sigma(A)\cap\sigma(B)\) where \sigma sands for \(\Fr,\Pr\)
      or \(\Sort\), and s.t. \(\Un(C)\subseteq\Un(A)\) and \(\Ex(C)\subseteq\Ex(B)\)
   #+END_exercise

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Let \(L(J)\) be a many-sorted language. A structure for \(L(J)\) is a pair \(\la D,\phi\ra\)
   where \(D\) is a set of non-empty sets \(\{D_j:j\in J\}\) and \phi is a map from the constants
   of \(L(J)\) into appropriate objects. We call \(D_j\) the domain of the structure of sort \(j\).
   An individual constant of sort \(j\) is a member of \(D_j\). Let \(\calm=\la D,\phi\ra\)
   and \(\calm'=\la D',\phi'\ra\) be two structures for \(L(J)\). We say \(\calm'\) is an extension
   of \(\calm\) and write \(\calm\subseteq\calm'\) if
   1. for each \(j\in J\), \(D_j\subseteq D_j'\)
   2. for each individual constant \(k\), \(\phi'k=\phi k\)
   3. for each predicate constant \(R\) with \((n;j_1,\dots,j_n)\) assigned to it
      \begin{equation*}
      \phi R=\phi'R\cap(D_{j_1}\times\cdots\times D_{j_n})
      \end{equation*}
   4. for each constant constant \(f\) with \((n;j_1,\dots,j_n,j)\) assigned to it and
      \((d_1,\dots,d_n)\in D_{j_1}\times\cdots\times D_{j_n}\)
      \begin{equation*}
      (\phi' f)(d_1,\dots,d_n)=(\phi f)(d_1,\dots,d_n)
      \end{equation*}


   A formula is said to be *existential* if \(\Un(A)\) is empty
   #+END_definition

   #+ATTR_LATEX: :options [Łoś-Tarski]
   #+BEGIN_corollary
   The following are equivalent: let \(A\) be a formula of an ordinary (i.e., single-sorted)
   language \(L\)
   1. For any structure \(\calm\) (for \(L\)) and extension \(\calm'\), and any
      assignments \(\phi,\phi'\) from the domains of \(\calm,\calm'\), respectively, which agree on the
      free variables of \(A\), if \((\calm,\phi)\) satisfies \(A\), then so does \((\calm',\phi')\)
   2. There exists an (essentially) existential formula \(B\) s.t. \(A\equiv B\) is provable and the
      free variable of \(B\) are among those of \(A\)

   #+END_corollary

   #+ATTR_LATEX: :options [Feferman]
   #+BEGIN_proof
   We assume (for simplicity) that the language has no individual and function constants.

   Let \(\calm\) and \(\calm'\) be two structures of the form
   \begin{equation*}
   \calm=\la D_1,\{R_i\}_{i\in I}\ra,\quad
   \calm'=\la D_2,\{R_i'\}_{i\in I}\ra
   \end{equation*}
   Let \(J\) be \(\{1,2\}\). \((J,I,\la k_i\ra_{i\in I})\) will determine a 'type' of structures.
   Let \(L^+\) be a corresponding language. It contains the original language \(L\) as the
   sublanguage of sort 1. For each bound variable \(u\), the \(n\)th bound variable of sort 1,
   let \(u'\) be the \(n\)th bound variable of sort 2. If \(C\) is an \(L\)-formula, then \(C'\)
   denotes the result of replacing each bound variable \(u\) in \(C\) by \(u'\);
   hence \(\Fr(C)=\Fr(C')\). With this notation, define \(\Ext\) to be the form
   \(\forall u'\exists u(u'=u)\). Then
   \begin{equation*}
   \Ext,\{\exists u_i'(u_i'=b_i)\}_{i=1}^n,A'\to A
   \end{equation*}
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Let \(R\) be a set and suppose a set \(W_p\) is assigned to every \(p\in R\).
   If \(R_1\subseteq R\) and \(f\in\prod_{p\in R_1}W_p\), then \(f\) is called a *partial function*
   (over \(R\)) with domain \(\dom(f)=R_1\). If \(\dom(f)=R\) then \(f\) is called a *total function*
   (over \(R\)). If \(f\) and \(g\) are partial functions and \(\dom(f)=D_0\subseteq\dom(g)\)
   and \(f(x)=g(x)\) for every \(x\in D_0\), then we call \(g\) an *extension* of \(f\) and
   write \(f\prec g\) and \(f=g\restriction D_0\)
   #+END_definition

   #+ATTR_LATEX: :options [a generalized Kőnig's lemma]
   #+BEGIN_proposition
   Let \(R\) be any set. Suppose a finite set \(W_p\) is assigned to every \(p\in R\). Let \(P\) be
   a property of partial functions \(f\) over \(R\) satisfying the following conditions:
   1. \(P(f)\) holds iff there exists a finite subset \(N\) of \(R\) satisfying \(P(f\restriction N)\)
   2. \(P(f)\) holds for every total function \(f\)


   Then there exists a finite subset \(N_0\)  of \(R\) s.t. \(P(f)\) holds for every \(f\) with \(N_0\subseteq\dom(f)\).
   #+END_proposition

   Note that \(R\) can have arbitrarily large cardinality. The case that \(R\) is the set of natural
   numbers is the original Kőnig's lemma.

   #+BEGIN_proof
   Let \(X=\prod_{p\in R}W_p\), and give each \(W_p\) the discrete topology, and \(X\) the product
   topology. Since each \(W_p\) is compact, so is \(X\) (Tychonoff's theorem). For each \(g\)
   s.t. \(\dom(g)\) is  finite, let
   \begin{equation*}
   N_g=\{f\mid f\text{ is total and }g\prec f\}
   \end{equation*}
   Let
   \begin{equation*}
   C=\{N_g\mid\dom(g)\text{ is finite and }P(g)\}
   \end{equation*}
   \(C\) is an open cover of \(X\). Therefore \(C\) has a finite subcover, say
   \begin{equation*}
   N_{g_1},\dots,N_{g_k}
   \end{equation*}
   Let \(N_0=\dom(g_1)\cup\dots\cup\dom(g_k)\). We will show that \(N_0\) satisfies the condition
   of the theorem. If \(N_0\subseteq\dom(g)\), then let \(g\prec f\), \(f\) total. Then \(P(f)\)
   and \(f\in N_{g_1}\cup\dots\cup N_{g_k}\). Say \(f\in N_{g_i}\). So \(g_i\prec f\), \(P(g_i)\)
   and \(g_i\prec g\). Therefore \(P(g)\).
   #+END_proof

   To simplify the discussion, we assume that our language does not contain individual or function
   constants.

   We deal with \(\LJ'\). \(\LJ'\) is defined by restricting \(\LK\) as follows: The
   inferences \(\neg\)right, \(\supset\)right and \(\forall\)right are allowed only when the
   principal formulas are the only formulas in the succedents of the lower sequents. (these are
   called the "critical inferences" of \(\LJ'\)).

   By interpreting a sequent of \(\LJ'\), say \(\Gamma\to B_1\dots,B_n\)
   as \(\Gamma\to B_1\vee\dots\vee B_n\), its a routine matter to prove that \(\LJ'\) and \(\LJ\)
   are equivalent.

   Starting with a given \(\Gamma\to\Delta\), we can carry out the reduction process which was
   defined in Lemma ref:lemma8.3 except that we omit the stages 1,7,9.

   The tree obtained by the above reduction process is called the reduction tree for \(\Gamma\to\Lambda\)
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Let \Gamma and \Delta be well-ordered sequences of formulas, which may be infinite. We say
   that \(\Gamma\to\Delta\) is *provable* (in \(\LJ'\)) if there are finite sequences of \Gamma and \Delta,
   say \(\tilde{\Gamma}\) and \(\tilde{\Delta}\), respectively, s.t. \(\tilde{\Gamma}\to\tilde{\Delta}\) is provable
   
   #+END_definition

* Peano Arithmetic

** A formulation of Peano arithmetic
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   The language of the system, which will be called Ln, contains finitely many constants, as follows
   - Individual constant: 0
   - Function constants: ',+,\(\cdot\)
   - Predicate constant: =
     where ' is unary while the other constants are binary


   A *numeral* is an expression of the form \(0^{\prime\dots\prime}\), i.e., zero followed by \(n\) primes for
   some \(n\), which is denoted by \(\barn\). Further, if \(s\) is a closed term of Ln denoting a
   number \(m\) (in the intended interpretation), then \(\bars\) denotes the numeral \(\barm\) (e.g.
   if \(s\) is \(\bar{2}+\bar{3}\) then \(\bars\) denotes \(\bar{5}\)) 
   #+END_definition
   
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   The first axiom system of Peano arithmetic which we consider,  CA, consists of \(\Gamma_e\) for
   Ln in definition ref:def7.3 and the following sentences

   | A1 | \(\forall x\forall y(x'=y'\supset x=y)\) |
   | A2 | \(\forall x(\neg x'=0)\)                    |
   | A3 | \(\forall x(x+0)=x\)                     |
   | A4 | \(\forall x \forall y (x+y'=(x+y)')\)          |
   | A5 | \(\forall x(x\cdot 0=0)\)                |
   | A6 | \(\forall x \forall y(x\cdot y'=x\cdot y+x)\)  |

   The second axiom system of Peano arithmetic which we consider VJ, consists of all sentences of
   the form
   \begin{equation*}
   \forall z_1 \dots \forall z_n \forall x(F(0,z)\vee \forall y(F(y,z)\supset F(y',z))\supset F(x,z))
   \end{equation*}
   where \(z\) is an abbreviation for the sentence of variables \(z_1,\dots,z_n\); and all the
   variables which are free in \(F(x,z)\) are among \(x,z\)

   The basic logical system of Peano arithmetic is \(\LK\). Then \(\CA\cup\VJ\) is an axiom system
   with equality. Furthermore \(\forall x\forall y(x=y\supset(F(x)\equiv y))\) is provable for every
   formula of Ln (cf. Proposition ref:prop7.2 )  
   #+END_definition


   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   The system \(\PA\) (Peano arithmetic) is obtained from \(\LK\) (in the language Ln) by adding
   extra initial sequents (called the *mathematical initial sequents*) and a new rule of inference
   called "*ind*", stated below
   1. Mathematical initial sequents: additional initial sequents of \(\LKe\) for Ln in Definition
      ref:def7.1 and the following sequents
      \begin{align*}
      &s'=t' \to s=t\\
      &s'=0 \to \\
      & \to s+0 = s\\
      & \to s+t'=(s+t)'\\
      & \to s\cdot 0=0\\
      & \to s\cdot t'=s\cdot t+s
      \end{align*}
      where \(s,t,r\) are arbitrary terms of Ln
   2. Ind:
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{F(a),\Gamma\to\Delta,F(a')}
      \infer1{F(0),\Gamma\to\Delta,F(s)}
      \end{prooftree}
      \end{equation*}
      where \(a\) is not in \(F(0)\), \Gamma or \Delta; \(s\) is an arbitrary term (which may contain \(a\));
      and \(F(a)\) is an arbitrary formula of Ln

      \(F(a)\) is called the *induction formula*, and \(a\) is called the *eigenvariable* of this
      inference. Further, we call \(F(a)\) and \(F(a')\) the *left* and *right auxiliary formula*,
      respectively, and \(F(0)\) and \(F(s)\) the *left* and *right principal formula*, respectively, of
      this inference.

      The initial sequents of the form \(D\to D\) are called *logical* initial sequents

      A *weak inference* is a structural inference other than cut.
   #+END_definition
 
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   A sequent is provable from \(\CA\cup\VJ\)(in \(\LK\)) iff it is provable in \(\PA\). Hence the
   axiom system \(\CA\cup\VJ\) is consistent iff \(\to\) is not provable in \(\PA\)
   #+END_proposition

   Thus we can restrict out attention to the system \(\PA\). In the rest of this chapter,
   "provability" means provability in \(\PA\).

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(P\) be a proof in \(\PA\) of a sequent \(S(a)\), where all the occurrence of \(a\)
   in \(S(a)\) are indicated. Let \(s\) be an arbitrary term. Then we may construct
   a \(\PA\)-proof \(P'\) of \(S(s)\) s.t. \(P'\) is regular (cf. Lemma ref:lemma2.9) and \(P'\)
   differs from \(P\) only in that some free variables are replaced by some other free variables and
   some occurrences of \(a\) are replaced by \(s\)
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   label:lemma9.6
   1. For an arbitrary closed term \(s\), there exists a unique numeral \(\barn\) s.t. \(s=\barn\)
      is provable without an essential cut (Definition ref:def7.5) and without ind
   2. Let \(s\) and \(t\) be closed terms. Then either \(\to s=t\) or \(s=t\to\) is provable without
      an essential cut or ind
   3. Let \(s\) and \(t\) be closed terms s.t. \(s=t\) is provable without an essential cut or ind
      and let \(q(a)\) and \(r(a)\) be two terms with some occurrences of \(a\) (possibly none).
      Then \(q(s)=r(s) \to q(t)=r(t)\) is provable without an essential cut or ind
   4. Let \(s\) and \(t\) be as in 3. For an arbitrary formula \(F(a):s=t,F(s)\to F(t)\) is provable
      without an essential cut or 
   #+END_lemma

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   label:def9.7
   When we consider a formula or a logical symbol together with the place that it occupies in a
   proof, in a sequent or in a formula, we refer to it as a formula or a logical symbol in the
   proof, in the sequent or in the formula. A formula in a sequent is also called a
   *sequent-formula*
   1. If a formula \(E\) is contained in the upper sequent of an inference using one of the rules of
      inference in ref:sec:one or "ind", then the *successor* of \(E\) is defined as follows
      1. If \(E\) is a cut formula, then \(E\) has no successor
      2. If \(E\) is an auxiliary formula of any inference other than a cut or exchange, then the
         principal formula is the successor of \(E\)
      3. If \(E\) is he formula denoted by \(C\) (respectively, \(D\)) in the upper sequent of an
         exchange (in Definition ref:def2.1), then the formula \(C\) (respectively, \(D\)) in the
         lower sequent is the successor of \(E\)
      4. If \(E\) is the \(k\)th formula of \Gamma,\Pi,\Delta or \Lambda in the upper sequent (in Definition
         ref:def2.1), then the \(k\)th formula of \Gamma,\Pi,\Delta or \Lambda, respectively, in the lower sequent is
         the successor of \(E\)
   2. A sequent formula is called an *initial formula* or an *end-formula* if it occurs, in an initial
      sequent or an end-sequent
   3. A sequent of formulas in a proof with the following properties is called a *bundle*
      1. The sequence begins with an initial formula or a weakening formula
      2. The sequence ends with an end-formula or a cut-formula
      3. Every formula in the sequence except the last is immediately followed by its successor
   4. Let \(A\) and \(B\) be formulas. \(A\) is called an *ancestor* of \(B\) and \(B\) is called a
      *descenent* of \(A\) if there is a bundle containing both \(A\) and \(B\) in which \(A\)
      appears above \(B\)
   5. Let \(A\) and \(B\)  be formulas. If \(A\) is the successor of \(B\), then \(B\) is called a
      *predecessor* of \(A\)
   6. A bundle is called *explicit* if it ends with an end formula

      It is called *implicit* if it ends with a cut-formula

      A formula in a proof is called explicit or implicit according as the bundles containing the
      formula are explicit or implicit

      A sequent in a proof is called explicit or implicit according as this sequent contains an
      implicit formula or not

      A logical inference in a proof is called explicit or implicit according as the principal
      formula of this inference is explicit or implicit
   7. The *end-piece* of a proof is defined as follows
      1. The end-sequent of the proof is contained in the end-piece
      2. The upper sequent of an inference other than an implicit logical inference is contained in
         the end-piece iff the lower sequent is contained in it
      3. The upper sequent of an implicit logical inference is not contained in it

      We can rephrase this definition as follows: A sequent in a proof is in the end-piece of the
      proof iff there is no implicit inference below this sequent
   8. An inference of a proof is said to be *in the end-piece* of the proof if the lower sequent of
      the inference is in the end-piece
   9. Let \(J\) be an inference in a proof. We say \(J\) *belongs to the boundary* (or \(J\) is a
      *boundary inference*) if the lower seuqent of \(J\) is in the end-piece and the upper sequent is
      not. It should be noted that if /\(J\) belongs to the boundary, then it is an implicit logical inference/.
   10. A cut in the end-piece is called *suitable* if each cut formula of this cut has an ancestor
       which is the principal formula of a boundary inference
   11. A cut is called *inessential* if the cut formula contains no logical symbol; otherwise it is
       called *essential*

       In \(\PA\), the cut formulas of inessential cuts are of the form \(s=t\)
   12. A proof \(P\) is *regular* if: 1. the eigenvariables of any two distinct inferences
       (\(\forall\)right, \(\exists\)left or induction) in \(P\) are disctinct from each other 2. if
       a free variable \(a\) occurs as an eigenvariable of a sequent \(S\) of \(P\), then \(a\) only
       occurs in sequents above \(S\)
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop9.8
   For an arbitrary proof of \(\PA\), there exists a regular proof of the same end-sequent, which
   can be obtained from the original proof by simply replacing free variables
   #+END_proposition

   #+BEGIN_proof
   Lemma ref:lemma2.10
   #+END_proof




   

** The Incompleteness Theorem
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   An axiom system \(\cala\) is said to be *axiomatizable* if there is a finite set of schemata
   s.t. \(\cala\) consists of all the instances of these schemata. A formal system \(\bS\) is called
   axiomatizable if there is an axiomatizable axiom system \(\cala\) s.t. \(\bS\) is equivalent
   to \(\LK_{\cala}\)

   A system \(\bS\) is called an extension of \(\PA\) if every theorem of \(\PA\) is provable in \(\bS\).
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   The class of primitive recursive functions is the smallest class of functions generated by the
   following schemata
   1. \(f(x)=x'\) , where \('\) is the successor function
   2. \(f(x_1,\dots,x_n)=k\), where \(n\ge1\) and \(k\) is a natural number
   3. \(f(x_1,\dots,x_n)=x_i\), where \(1\le i\le n\)
   4. \(f(x_1,\dots,x_n)=g(h_1(x_1,\dots,x_n),\dots,h_m(x_1,\dots,x_n))\), where \(g,h_1,\dots,h_m\)
      are primitive recursive functions
   5. \(f(0)=k,f(x')=g(x,f(x))\) where \(k\) is a natural number and \(g\) is a primitive recursive function
   6. \(f(0,x_2,\dots,x_n)=g(x_2,\dots,x_n)\),
      \(f(x',x_2,\dots,x_n)=h(x,f(x,x_2,\dots,x_n),x_2,\dots,x_n)\), where \(g\) and \(h\) are
      primitive recursive functions

      An \(n\)-ary relation \(R\) is said to be primitive recursive if there is a primitive
      recursive function \(f\) which assumes values 0 and 1 only s.t. \(R(a_1,\dots,a_n)\) is true
      iff
      \(f(a_1,\dots,a_n)=0\)
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   label:lemma10.5
   The consistency of \(\bS\) (i.e., \(\bS\)-unprovability of \(\to\)) is equivalent to
   the \(\bS\)-unprovability of \(0=1\) (cf. Proposition ref:prop4.2)
   #+END_lemma

   #+ATTR_LATEX: :options [Gödel]
   #+BEGIN_proposition
   label:prop10.6
   1. The graphs of all the primitive recursive functions can be expressed in Ln, so that their
      defining equations are provable in \(\PA\)

      Thus the theory of primitive recursive functions can be translated into our formal system of
      arithmetic. We may therefore assume that \(\PA\) (or any of its extensions) actually contains
      the function symbols for primitive recursive functions and their defining equations, as well
      as predicate symbols for the primitive recursive relations
   2. Let \(R\) be a primitive recursive relation of \(n\) arguments. It can be represented
      in \(\PA\) by a formula \(\barR(a_1,\dots,a_n)\), namely \(\barf(a_1,\dots,a_n)=\bar{0}\),
      where \(f\) is the characteristic function of \(R\). Then for any \(n\)-tuple of
      numbers \((m_1,\dots,m_n)\), if \(R(m_1,\dots,m_n)\) is true,
      then \(\barR(\barm_1,\dots,\barm_n)\) is \(\PA\)-provable
   #+END_proposition

   #+BEGIN_proof
   Follow this [[http://www.cs.cornell.edu/courses/cs4860/2009sp/lec-22.pdf][note]].


    2. [@2] We prove that for any primitive recursive function \(f\) (of \(n\) arguments) and any
    numbers \(m_1,\dots,m_n\), \(p\), if \(f(n_1,\dots,m_n)=p\),
    then \(\barf(\barm_1,\dots,\barm_n)=\barp\) is \(\PA\)-provable. The proof is by induction on
    the construction of \(f\).
   #+END_proof

   The converse proposition (i.e. for primitive recursive \(R\), if \(\barR(\barm_1,\dots,\barm_n)\)
   is \(\PA\)-provable, then \(R(m_1,\dots,m_n)\) is true) follows from the consistency of \(\PA\)

   #+ATTR_LATEX: :options [Gödel numbering]
   #+BEGIN_definition
   For an expression \(X\), we use \(\ucorner{X}\) to denote the corresponding number, which we call
   the Gödel number of \(X\)
   1. First assign different odd numbers to the symbols of Ln (We include \(\to\) and \(-\) among
      the symbols of the language here)
   2. Let \(X\) be a formal expression \(X_0X_1\dots X_n\), where each \(X_i\), \(0\le i\le n\) is a
      symbol of L. Then \(\ucorner{X}\) is defined to
      be \(2^{\ucorner{X_1}}3^{\ucorner{X_1}}\dots p_n^{\ucorner{X_n}}\), where \(p_n\) is
      the \(n\)th prime number
   3. If \(P\) is a proof of the form
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{Q}
      \infer1{S}
      \end{prooftree} \quad\text{ or }\quad
      \begin{prooftree}%[center=false]
      \hypo{Q_1}
      \hypo{Q_2}
      \infer2{S}
      \end{prooftree}
      \end{equation*}
      hen \(\ucorner{P}\) is \(2^{\ucorner{Q}}3^{\ucorner{-}}5^{\ucorner{S}}\) or
      \(2^{\ucorner{Q_1}}3^{\ucorner{Q_2}}5^{\ucorner{-}}7^{\ucorner{S}}\) respectively


   If an operation or relation defined on a class of formal objects is thought of in terms of the
   corresponding number-theoretic operation or relation on their Gödel numbers, we say that the
   operation or relation has been *arithmetized*. More precisely, suppose \psi is an operation defined
   on \(n\)-tuples of formal objects of a certain class, and \(f\) is a number-theoretic function
   s.t. for all formal objects \(X_1,\dots,X_n,X\) if \psi applied to \(X_1,\dots,X_n\) produces \(X\),
   then \(f(\ucorner{X_1},\dots,\ucorner{X_n})=\ucorner{X}\). Then \(f\) is called the
   *arithmetization* of \psi
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   label:lemma10.8
   1. The operation of substitution can be arithmetized primitive recursively, i.e., there is a
      primitive recursive function \(\sb\) of two arguments s.t. if \(X(a_0)\) is an expression of L
      (where all occurences of \(a_0\) in \(X\) are indicated), and \(Y\) is another expression,
      then \(\sb(\ucorner{X(a_0)},\ucorner{Y})=\ucorner{X(Y)}\) where \(X(Y)\) is the result of
      substituting \(Y\) for \(a_0\) and \(X\)
   2. There is a primitive recursive function \(\nu\) s.t.
      \(v(m)=\ucorner{\text{the $m$th numeral}}\). That is, \(\nu(m)=\ucorner{\barm}\).
   3. The notion that \(P\) is a proof (of the system \(\bS\)) of a formula \(A\) (or a
      sequent \(S\)) is arithmetized primitive recursively; i.e. there is a primitive recursive
      relation \(\Prov(p,a)\) s.t. \(\Prov(p,a)\) is true iff there is a proof \(P\) and a
      formula \(A\) (or a sequent \(S\)) s.t. \(p=\ucorner{P}\), \(a=\ucorner{A}\)
      (or \(a=\ucorner{S}\)) and \(P\) is a proof of \(A\) (or \(S\))
   4. \(\Prov\) may be written as \(\Prov_{\bS}\) to emphasize the system \(\bS\)
   5. the formal expression for \(\Prov\) will be denoted by \(\ove{\Prov}\)
   #+END_lemma

   \(\exists x\ove{\Prov}(x,\ove{\ucorner{A}})\) is often abbreviated to \(\ove{\Pr}(\ove{\ucorner{A}})\)
   or \(\vdash\ove{\ucorner{A}}\)

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop10.9
   1. If \(A\) is \(\bS\)-provable, then \(\vdash\ove{\ucorner{A}}\) is \(\bS\)-provable
   2. If \(A\leftrightarrow B\) is \(\bS\)-provable,
      then \(\ove{\Pr}(\ove{\ucorner{A}})\leftrightarrow\ove{\Pr}(\ove{\ucorner{B}})\) is \(\bS\)-provable
   3. \(\vdash\ove{\ucorner{A}}\to(\vdash\ove{\ucorner{\vdash\ove{\ucorner{A}}}})\) is \(\bS\)-provable
   #+END_proposition

   #+BEGIN_proof
   1. Suppose \(A\) is provable with a proof \(P\). Then by 3 of Lemma ref:lemma10.9,
      \(\Prov(\ucorner{P},\ucorner{A})\) is true, which by 2 of Proposition ref:prop10.6,
      that \(\exists x\ove{\Prov}(x,\ove{\ucorner{A}})\), i.e., \(\vdash\ove{\ucorner{A}}\) is \(\bS\)-provable.
   2. Suppose \(A\equiv B\) is provable with a proof \(P\) and \(A\) is provable with a proof \(Q\).
      There is a prescription for constructing a proof of \(B\) from \(P\) and \(Q\), which can be
      arithmetized by a primitive recursive fucntion \(f\).
      Thus \(\Prov(q,\ucorner{A})\to\Prov(f(p,q),\ucorner{B})\) is true, from which it follows by
      Proposition ref:prop10.6 that \(\vdash\ove{\ucorner{A}}\to\vdash\ove{\ucorner{B}}\) is
      provable.
   3. If \(P\) is a proof of \(A\), then we can construct a proof \(Q\) of \(\vdash\ovecor{A}\)
      by 1. This process is uniform in \(P\); in other words, there is a uniform prescription for
      obtaining \(Q\) from \(P\). Thus
      \begin{equation*}
      \Prov(p,\ucorner{A})\to\Prov(f(p),\ucorner{\ove{\Pr}(\ovecor{A})})
      \end{equation*}
      is true for some primitive recursive function \(f\), from which it follows that
      \(\vdash\ovecor{A}\to\vdash\ovecor{\vdash \ovecor{A}}\)
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A formula of L (the language of \(\bS\)) with one free variable, say \(T(a_0)\), is called a
   *truth definition* for \(\bS\) if for every sentence of \(A\) of L
   \begin{equation*}
   T(\ovecor{A})\equiv A
   \end{equation*}
   is \(\bS\)-provable
   #+END_definition

   #+ATTR_LATEX: :options [Tarski]
   #+BEGIN_theorem
   <<Problem3>>
   label:thm10.11
   If \(\bS\) is consistent, then it has no truth definition
   #+END_theorem

   #+BEGIN_proof
   Suppose otherwise. Consider the formula \(F(a_0)\), with sole free variable \(a_0\), defined
   as: \(\neg T(\ove{\sb}(a_0,\barnu(a_0)))\). Put \(p=\ucorner{F(a_0)}\), and let \(A_T\) be the
   sentecne \(F(\barp)\). Then by definition
        \begin{equation*}
        A_T\equiv\neg T(\ove{\sb}(\barp,\barnu(\barp)))
        \end{equation*}
   Also since \(\ucorner{A_T}=\sb(p,\nu(p))\) by definition,
   (note that \(\ucorner{F(\barp)}=\sb(p,\nu(p))\))
   we can prove in \(\bS\) the equivalences
        \begin{align*}
        A_T&\equiv T(\ovecor{A_T})\quad(\text{by assumed property of $T$})\\
        &\equiv T(\ove{\sb}(\barp,\barnu(\barp)))
        \end{align*}
   #+END_proof

   In the proof of Theorem ref:thm10.11 we need /not/ asume that \(\bS\) is axiomatizable. So we may
   take as the axioms of \(\bS\) the set of all sentences of Ln which are /true/ in the intended
   interpretation \(\fM\) of \(\PA\). We then obtain that there is no formula \(T(a_0)\) of Ln s.t.
   for any sentence \(A\)of Ln
   \begin{equation*}
   A\text{ is true }\Leftrightarrow T(\ovecor{A})\text{ is true}
   \end{equation*}
   The corollary of Theorem ref:thm10.11 can be stated in the form: "The notion of arithmetical
   truth is not arithmetical"
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   \(\bS\) is called *incomplete* if for some sentence \(A\), neither \(A\) nor \(\neg A\) is provable in \(\bS\)
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Consider a formula \(F(\alpha)\) with a metavariable \alpha (i.e. a new predicate variable, not in L, which
   we only use temporarily for notational convenience), where \alpha is regared as an atomic formula
   in \(F(\alpha)\) and \(F(\alpha)\) is closed. \(F(\vdash \ove{\sb}(a_0,\barnu(a_0)))\) is a formula with \(a_0\)
   as its sole free variable. Define \(p=\ucorner{F(\vdash \ove{\sb}(a_0,\barnu(a_0)))}\) and \(A_F\) as
   \(F(\vdash \ove{\sb}(\barp,\barnu(\barp)))\). Note that \(A_F\) is a sentence of L
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   label:lemma10.14
   \(A_F\equiv F(\vdash \ovecor{A_F})\) is provable in \(\bS\)
   #+END_lemma

   #+BEGIN_proof
   Since \(\ucorner{A_F}=\sb(p,\nu(p))\) by definition
   \begin{equation*}
   \ovecor{A_F}=\ovesb(\barp,\barnu(\barp))\text{ is provable in }\bS
   \end{equation*}
   Hence \(A_F\equiv F(\vdash \ovecor{A_F})\) is provable in \(\bS\)
   #+END_proof

   From now no we shall use the abbreviation \(\vdash A\) for \(\vdash\ovecor{A}\)

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   \(\bS\) is called *\(\omega\)-consistent* if the following condition is satisfied. For every
   formula \(A(a_0)\), if \(\neg A(\barn)\) is provable in \(\bS\) for every \(n\in\N\),
   then \(\exists xA(x)\) is not provable in \(\bS\). Note that \(\omega\)-consistency of \(\bS\) implies
   consistency of \(\bS\)
   #+END_definition

   #+ATTR_LATEX: :options [Gödel's first incompleteness theorem]
   #+BEGIN_theorem
   label:thm10.16
   If \(\bS\) is \(\omega\)-consistent, then \(\bS\) is incomplete
   #+END_theorem

   #+BEGIN_proof
   There exists a sentence \(A_G\) of L s.t. \(A_G\equiv\neg\vdash A_G\) is provable in \(\bS\).
   (Any such sentence will be called a Gödel sentence for \(\bS\).) This follows from Lemma
   ref:lemma10.14 by taking \(F(\alpha)\) to be \(\neg\alpha\).

   First we shall show that \(A_G\) is not provable in \(\bS\), assuming only the consistency
   of \(\bS\) (but without assuming the \(\omega\)-consistency of \(\bS\)). Suppose that \(A_G\)
   where provable in \(\bS\). Then by 1 of Proposition ref:prop10.9, \(\vdash A_G\) is provable
   in \(\bS\); thus by the definition of Gödel sentence, \(\neg A_G\) is provable in \(\bS\)

   Next we shall show that \(\neg A_G\) is not provable in \(\bS\), assuming the \(\omega\)-consistency
   of \(\bS\). Since we have proved that \(A_G\) is not provable in \(\bS\), for
   each \(n=0,1,2,\dots\), \(\neg\ove{\Prov}(\barn,\ovecor{A_G})\) is provable in \(\bS\). By
   the \(\omega\)-consistency of \(\bS\), \(\exists x\ove{\Prov}(x,\ovecor{A_G})\) is not provable
   in \(\bS\). Since \(\neg A_G\equiv\vdash A_G\) is provable in \(\bS\), \(\neg A_G\) is not provable in \(\bS\)
   #+END_proof

   In fact ,\(A_G\), although unprovable, is (intuitively) true, since it asserts its own unprovability

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
    \(\oveConsis_{\bS}\) is the sentence \(\neg\vdash0=1\) (So \(\oveConsis_{\bS}\) asserts the
    consistency of \(\bS\))
   #+END_definition

   #+ATTR_LATEX: :options [Gödel's second incompleteness theorem]
   #+BEGIN_theorem
   If \(\bS\) is consistent, then \(\oveConsisS\) is not provable in \(\bS\)
   #+END_theorem

   #+BEGIN_proof
   Let \(A_G\) be a Gödel sentence. In the proof of Theorem ref:thm10.16, we proved that \(A_G\) is
   not provable, assuming only consistency of \(\bS\). Now we shall prove a stronger theorem: that
   \(A_G\equiv\oveConsisS\) is provable in \(\bS\)
   1. To show \(A_G\to\oveConsisS\) is provable in \(\bS\). By Lemma ref:lemma10.5,
      \(\neg\oveConsisS\equiv\forall\ucorner{A}(\vdash A)\) is provable. Therefore
      \begin{equation*}
    A_G\to\neg\vdash A_G\to\neg\forall\ucorner{A}(\vdash A)\to\oveConsisS
      \end{equation*}
   2. To show \(\oveConsisS\to A_G\) is provable in \(\bS\). Again by Lemma ref:lemma10.5,
      \(\oveConsisS,\vdash A_G\to\neg\vdash\neg A_G\to\neg\vdash\vdash A_G\)
      since \(\neg A_G\equiv\vdash A_G\). But \(\vdash A_G\to\vdash\vdash A_G\), by Proposition
      ref:prop10.9. So \(\oveConsisS,\vdash A_G\to\neg\vdash\vdash A_G\wedge\vdash\vdash A_G\) and so
      \(\oveConsisS\to\neg\vdash A_G\to A_G\)
   #+END_proof

** A Discussion of Ordinals from a Finitist Standpoint
   | O1 | 0 is an ordinal                                       |
   | O2 | Let \mu and \(\mu_1,\dots,\mu_n\) be ordinals. Then     |
   |    | \(\mu_1+\dots+\mu_n\) and \(\omega^{\mu}\) are ordinals      |
   | O3 | Only those objects obtained by O1 and O2 are ordinals |

   \(\omega^0\) will be dnoted by 1.
   1. < is a linear ordering and 0 is its least element
   2. \(\omega^{\mu}<\omega^{\nu}\) iff \(\mu<\nu\)
   3. Let \mu be an ordinal containing an occurrence of the symbol 0 but not 0 itself, and
      let \(\mu'\) be the ordinal obtained from \mu by eliminating this occurrence of 0 as  well as
      excessive occurrence of +. Then \(\mu=\mu'\)


    As a consequence of 3 it can be easily shown that
   4. [@4] Every ordinal which is not 0 can be expressed in the form
      \begin{equation*}
    \omega^{\mu_1}+\omega^{\mu_2}+\dots+\omega^{\mu_n}
      \end{equation*}
      where each of \(\mu_1,\dots,\mu_n\) which is not 0 has the same property. (Each
      term \(\omega^{\mu_i}\) is called a monomial of this ordinal)
   5. Let \mu and \nu be of the forms
      \begin{equation*}
      \omega^{\mu_1}+\dots+\omega^{\mu_k}\quad\text{ and }\quad
      \omega^{\nu_1}+\dots+\omega^{\nu_l}
      \end{equation*}
      respectively. Then \(\mu+\nu\) is defined as
      \begin{equation*}
      \omega^{\mu_1}+\dots+\omega^{\mu_k}+
      \omega^{\nu_1}+\dots+\omega^{\nu_l}
      \end{equation*}
   6. Let \mu be an ordinal which is written in the form of 4 and contains two consecutive
      terms \(\omega^{\mu_j}\) and \(\omega^{\mu_{j+1}}\) with \(\mu_j<\mu_{j+1}\), i.e., \mu is of the form
      \begin{equation*}
      \dots+\omega^{\mu_j}+\omega^{\mu_{j+1}}+\dots
      \end{equation*}
      and let \(\mu'\) be an ordinal obtained from \mu by deleting "\(\omega^{\mu_j}+\)", so
      that \(\mu'\) is of the form
      \begin{equation*}
      \dots+\omega^{\mu_{j+1}}+\dots
      \end{equation*}
      Then \(\mu=\mu'\)


   As a consequence of 6 we can show that
   7. [@7] For every ordinal \mu  (which is not 0) there is an ordinal of the form
      \begin{equation*}
      \omega^{\mu_1}+\dots+\omega^{\mu_n}
      \end{equation*}
      where \(\mu_1\ge\dots\ge\mu_n\) s.t. \(\mu=\omega^{\mu_1}+\dots+\omega^{\mu_n}\). This is
      called the normal form of \mu
   8. Let \mu have the normal form
      \begin{equation*}
      \omega^{\mu_1}+\dots+\omega^{\mu_n}
      \end{equation*}
      and \nu be \(>0\). Then \(\mu\cdot\omega^{\nu}=\omega^{\mu_1+\nu}\)
   9. Let \mu and \nu be as in 5. Then
      \begin{equation*}
      \mu\cdot\nu=\nu\cdot\omega^{\nu_1}+\dots+\mu\cdot\omega^{\nu_l}
      \end{equation*}
   10. \((\omega^\mu)^n\) is defiend as \(\omega^\mu\cdots\omega^\mu\) (\(n\) times) for any natural
       number \(n\). Then \((\omega^\mu)^n=\omega^{\mu\cdot n}\)


   #+BEGIN_center
   (*) Whenever a concrete method of constructing decreasing sequences of ordinals is given, any
   such decreasing sequence must be finite
   #+END_center


   Suppose \(a_0>a_1>\dots\) is a decreasing sequence concretely given
   1. Assume \(a_0<\omega\), or \(a_0\) is a natural number
   2. Suppose each \(a_i\) in \(a_0>a_1>\dots\) is written in the canonical form; \(a_i\) has the
      form
      \begin{equation*}
      \omega^{\mu_1^i}+\omega^{\mu_2^i}+\dots\dots+\omega^{\mu_{n_i}^i}+k_i
      \end{equation*}
      where \(\mu_j^i>0\) and \(k_i\) is a natural number. A sequence in which \(k_i\) does not
      appear for any \(a_i\) will be called 1-sequence. We call \(\omega^{\mu_1^i}+\dots+\omega^{\mu_{n_i}^i}\)
      in \(a_i\) the 1-major part of \(a_i\). We shall give a concrete method \((M_1)\) which
      enables us to do the following: given a decreasing sequence \(a_0>a_1>\dots\), where
      each \(a_i\) is written in its canonical form, the method \(M_1\) concretely produces a
      (decreasing) 1-sequence \(b_0>b_1>\dots\) so as to satisfy the condition

      #+BEGIN_center
      (\(\text{C}_1\)) \(b_0\) is the 1-major part of \(a_0\), and we can concretely show that
      if \(b_0>b_1>\dots\) is a finite sequence, then so is \(a_0>a_1>\dots\)
      #+END_center

      This method \(M_1\) (a 1-eliminator) is defined as follows. Put \(a_i=a_i'+k_i\),
      where \(a_i'\) is the 1-major part of \(a_i\). Then \(a_0>a_1>\dots\) can be expressed by
      \(a_0'+k_0>a_1'+k_1>\dots\)

      Put \(b_0=a_0'\). Suppose \(b_0>b_1>\dots>b_m\) has been constructed in such a manner
      that \(b_m\) is \(a_j'\) for some \(j\). Then either \(a_j'=a_{j+1}' =\dots =a_{j+p}'\) for
      some \(p\) and \(a_{j+p}\) is the last term in the sequence,
      or \(a_j'=a_{j=1}' =\dots =a_{j+p}'>a_{j+p+1}'\). This is so,
      since \(a_j' =a_{j+1}' =\dots =a_{j+p}' =\dots\) implies \(k_j>k_{j+1}>\dots>k_{j+p}>\dots\),
      but since a sequence (of natural numbers) must stop. Therefore either the whole sequence stops
      or \(a_{j+p}'>a_{j+p+1}'\) for some \(p\). If the former is the case, then stop. If the latter
      holds then put \(b_{m+1}=a_{j+p+1}'\)

      From the definition, it is obvious that \(b_0>b_1>\dots>b_m>\dots\). Suppose this sequence is
      finite, say \(b_0>b_1>\dots>b_m\). Then according to the prescribed construction
      of \(b_{m+1}\) the original sequence is finite. Then (\(\text{C}_1\)) is satisfied.
   3. Suppose we are given a decreasing sequence \(a_0>a_1>\dots\) in which \(a_0<\omega^2\). Then by a
      1-eliminator \(M_1\) applied to this sequence we can construct a 1-sequence \(b_0>b_1>\dots\)
      where \(b_0\le a_0\). Then \(b_0>b_1>\dots\) can be written in the
      form \(\omega\cdot k_0>\omega\cdot k_1>\dots\), which implies \(k_0>k_1>\dots\). Then by
      1, \(k_0>k_1>\dots\) must be finite, which implies \(b_0>b_1>\dots\) and \(a_0>a_1>\dots\) are finite.
   4. We now define "\(n\)-sequences" as follows. Let \(a_0>a_1>\dots\) be a descending sequence
      which is written in the form \(a_0'+c_0>a_1' +c_1>\dots\) where if \( a_i=a_i'+c_i\), then
      each monomial in \(a_i'\) is \(\ge\omega^n\) and each monomial in \(c_i\) is \(<\omega^n\)
      (\(a_i'\) is called the \(n\)-major part of \(a_i\)) Such a sequence is called
      an \(n\)-sequence if every \(c_i\) is empty

      Now assume (as an inductive hypothesis) that any descending sequence \(d_0>d_1>\dots\),
      with \(d_0<\omega^n\) is finite. We shall define a concrete method \(M_n\) (an \(n\)-eliminator)
      s.t., given a decreasing sequence \(a_0>a_1>\dots\), \(M_n\) concretely produces
      an \(n\)-sequence, say \(b_0>b_1>\dots\), which satisfies

      #+BEGIN_center
      (\(\text{C}_n\)) \(b_0\) is the \(n\)-major part of \(a_0\), and if \(b_0>b_1>\dots\) is
      finite then we can concretely show that \(a_0>a_1>\dots\) is finite
      #+END_center

      The prescription for \(M_n\) is as follows. Write each \(a_i\) as \(a_i'+c_i\) where \(a_i'\)
      is the \(n\)-major part of \(a_i\). Put \(b_0=a_0'\). Suppose \(b_0>b_1>\dots>b_m\) has been
      constructed and \(b_m\) is \(a_j'\). If \(a_j'=a_{j+1}'=\dots=a_{j+p}'\) and \(a_{j+p}'\) is the
      last term in the given sequence, then stop.
      Otherwise \(a_j'=a_{j+1}'=\dots=a_{j+p}'>a_{j+p+1}'\) for some \(p\),
      since \(a_j'=a_{j+1}' =\dots=a_{j+p}'\) implies that \(c_j>c_{j+1}>\dots>c_{j+p}\), which, by
      the induction hypothesis, is finite; hence for some \(p\), \(c_{j+p+1}\ge c_{j+p}\), which
      implies \(a_{j+p}'>a_{j+p+1}'\). Then define \(b_m=a_{j+p+1}'\). Then the
      sequence \(b_0>b_1>\dots\) satisfies (\(\text{C}_n\)).
   5. By means of the \(n\)-eliminator \(M_n\) we shall prove that a decreasing
      sequence \(a_0>a_1>\dots\), where \(a_0<\omega^{n+1}\) must be finite. By applying \(M_n\)
      to \(a_0>a_1>\dots\) we can construct concretely an \(n\)-sequence, say \(b_0>b_1>\dots\)
      where \(b_0\le a_0\). Moreover \(b_i\) can be written as \(\omega^n\cdot k_i\), where \(k_i\) is a
      natural number. So \(\omega^n\cdot k_0>\omega^n\cdot k_1>\dots\) and this implies \(k_0>k_1>\dots\),
      which is a finite sequence by 1, hence \(b_0>b_1>\dots\) is finite, which in turn implies
      that \(a_0>a_1>\dots\) with \(a_0<\omega^n\) is finite
   6. From 3 and 5 we conclude: given (concretely) any natural number \(n\), we can concretely
      demonstrate that any decreasing sequence \(a_0>a_1>\dots\) with \(a_0<\omega^n\)
   7. Any decreasing sequence \(a_0>a_1>\dots\) is finite if \(a_0<\omega^{\omega}\)
   8. Now the general theory of \(\alpha\)-sequences and \((\alpha,n)\)-eliminators will be developed,
      where \alpha ranges over all ordinals \(<\epsilon\) and \(n\) ranges over natural numbers \(>0\). A
      descending sequence \(d_0>d_1>\dots\) is called an *\(\alpha\)-sequence* if in each \(d_i\) all
      the monomials are \(\ge\omega^{\alpha}\). If \(a=a'+c\) where each monomial in \(a'\)
      is \(\ge\omega^{\alpha}\) and each monomial in \(c\) is \(<\omega^{\alpha}\), then we say that \(a'\) is
      the \(\alpha\)-major part of \(a\). An *\(\alpha\)-eliminator* has the property that given any
      concrete descending sequence, say \(a_0>a_1>\dots\), it concretely produces
      an \(\alpha\)-sequence
      \(b_0>b_1>\dots\) s.t.
      1. \(b_0\) is the \(\alpha\)-major part of \(a_0\)
      2. if \(b_0>b_1>\dots\) is a finite sequence then we can concretely demonstrate
         that \(a_0>a_1>\dots\) is finite

         Assuming that an \(\alpha\)-eliminator has been defined for every \alpha, we can show that any
         decreasing sequence is finite. For consider \(a_0>a_1>\dots\). There exists an \alpha
         s.t. \(a_0<\omega^{\alpha+1}\). An \(\alpha\)-eliminator concretely gives
         an \(\alpha\)-sequence \(b_0>b_1>\dots\) satisfying 1 and 2 above. Since \(b_0\le a_0\),
         each \(b_i\) can be written in the form \(\omega^\alpha\cdot k_i\); thus \(a_0>a_1>\dots\) is
         finite. This proves our objective (*). Therefore what must be done is to
         define \(\alpha\)-eliminators for all \(\alpha<\epsilon\)
   9. We rename an \(\alpha\)-eliminator to be an \((\alpha,1)\)-eliminator. Suppose
      that \((\alpha,n)\)-eliminators have benn defined. A \((\beta,n+1)\)-eliminator is a *concrete* method
      for constructing an \((\alpha\cdot\omega^\beta,n)\)-eliminator from any given \((\alpha,n)\)-eliminator.
   10. Suppose \(\{\mu_m\}_{m<\omega}\) is an increasing sequence of ordinals whose limit is \mu (where
       there is a concrete method for obtaining \(\mu_m\) for each \(m\)), and suppose \(g_m\) is
       an \(\mu_m\)-eliminator. Then the \(g\) defined as follows is a \(\mu\)-eliminator.

       Suppose \(a_0>a_1>\dots\) is a concretely given sequence. If \(a_0\) is written
       as \(a_0'+c_0\), where \(a_0'\) is the \(\mu\)-major part of \(a_0\) then there exists
       an \(m\) for which \(c_0<\omega^{\mu_m}\), so we may assume that each \(a_i\) is written
       as \(a_i'+c_i\), where \(a_i'\) is the \(\mu_m\)-major part of \(a_i\). Then \(g_m\) can be
       applied to the sequence \(a_0>a_1>\dots\) and hence it concretely produces
       a \(\mu_m\)-sequence
       \begin{equation}
    \label{eq:2.11.1}
    b_{10}>b_{11}>b_{12}>\dots
       \end{equation}
       satisfying (a) and (b) above (with \(\mu_m\) in place of \alpha), with \(b_{10}=a_0'\) so that in
       fact \(B_{10}\) is the \(\mu\)-major part of \(a_0\). Write \(b_0=b_{10}\).

       Now consider the sequence \(b_{11}>b_{12}>\dots\). Suppose \(b_{11}\ge\omega^{\mu}\). Then
       repeat the above procedure: i.e., for the sequence eqref:eq:2.11.1,
       write \(b_{10}=b_{10}'+c_{10}\), where \(b_{10}'\) is the \(\mu\)-major part of \(b_{10}\).
       Then there exists an \(m_1\) s.t. \(c_{10}<\omega^{m_1}\). So apply \(g_{m_1}\) to the sequence
       \(b_{11}>b_{12}>b_{13}>\dots\) to obtain a \(\mu_{m_1}\)-sequence
       \begin{equation*}
    b_{21}>b_{22}>b_{23}>\dots
       \end{equation*}
       satisfying (a) and (b), with \(b_{21}\) the \(\mu\)-major part of \(b_{10}\).
       Put \(b_1=b_{21}\). Suppose \(b_{22}\ge\omega^\mu\). Then repeat this procedure with the
       sequence \(b_{22}>b_{23}>\dots\) to obtain a sequence
       \begin{equation*}
    b_{32}>b_{33}>b_{34}>\dots
       \end{equation*}
       and put \(b_2=b_{32}\). Continuing in this way, we obtain a \(\mu\)-sequence
       \begin{equation*}
    b_0>b_1>b_2>\dots
       \end{equation*}
       If this sequence is finite with last term \(b_l=b_{l+1,l}\), then it follows that in this
       sequence
       \begin{equation*}
    \label{eq:2.11.2}
    b_{l+1,l}>b_{l+1,l+1}>b_{l+1,l+2}>\dots
       \end{equation*}
       We must have \(b_{l+1,l+1}<\omega^\mu\). So \(b_{l+1,l+1}<\omega^{\mu_{m'}}\) for some \(m'\).
       Applying \(g_{m'}\) to this sequence eqref:eq:2.11.2, we obtain a
       finite \(\mu_{m'}\)-sequence with only the term 0; hence the
       sequence \(b_{l,l-1}>b_{l,l}>\dots\) is finite; then \(b_{l-1,l-2}>b_{l-1,l-1}>\dots\) is
       finite; hence \(a_0>a_1>\dots\) is finite
   11. Suppose \(\{\mu_m\}_{m<\omega}\) is a sequence of ordinals whose limit is \mu and suppose for
       each \(m\), a \((\mu_m,n+1)\)-eliminator is concretely given. Then we can define
       a \((\mu,n+1)\)-eliminator \(g\) as follows. The difinition is by induction on \(n\).

       For \(n=0\), 10 applies.

       Assume 11 for \(n\); so there is an operation \(k_n\) s.t. for any
       sequence \(\{\gamma_m\}_{m<\omega}\) with limit \gamma
       and \((\gamma_m,n)\)-eliminator \(g_m'\), \(k_n\) applied to \(g_m'\) concretely produces
       a \((\gamma,n)\)-eliminator. Now for \(n+1\), suppose a sequence \(\{\beta_m\}_{m<\omega}\) with limit
       \beta and an \((\alpha,n)\)-eliminator \(p\) are given. Since \(g_m\) is
       a \((\beta_m,n+1)\)-eliminator, it produces concretely
       an \((\alpha\cdot\omega^{\beta_m},n)\)-eliminator from \(p\), which we denote by \(g_m(p)\).
       So by taking \(\alpha\cdot\omega^{\beta_m}\) for \(\gamma_m\), \(g_m(p)\) for \(g_m'\)
       and \(\alpha\cdot\omega^\beta\) for \gamma, we can apply the inductive hypothesis; thus \(k_n\)
       applied to \(\{g_m\}'\) defines an \((\alpha\cdot\omega^\beta,n)\)-eliminator \(q\). This
       procedure for defining \(q\) from \(p\) is concrete, and so serves as a \((\beta,n+1)\)-eliminator.
   12. Suppose \(g\) is a \((\mu,n+1)\)-eliminator. Then we will construct
       a \((\mu\cdot\omega,n+1)\)-eliminator.  In virtue of 11 it suffices to show that we can
       concretely construct (from \(g\)) a \((\mu\cdot m,n+1)\)-eliminator for every \(m<\omega\).
       Suppose an \((\alpha,n)\)-eliminator, say \(f\), is given. Note that
       \begin{equation*}
    \alpha\cdot\omega^{\mu\cdot m}=\alpha\cdot
    \underbrace{\omega^\mu\cdot\omega^\mu\dots\omega^\mu}_{m}
       \end{equation*}
       Since \(g\) is a \((\mu,n+1)\)-eliminator, \(g\) concretely constructs
       an \((\alpha\cdot\omega^\mu,n)\)-eliminator from \(f\), which we denote by \(g(f)\). Now apply
       \(g\) to this, to obtain
       an \((\alpha\cdot\omega^\mu\cdot\omega^\mu,n)\)-eliminator \(g(g(f))\). Repeating this
       procedure \(m\) times, we obtain
       the \((\alpha\cdot\omega^{\mu\cdot m}),n\)-eliminator \(g(g(\cdots g(f)\cdots))\)
   13. We can construct a \((1,m+1)\)-eliminator for every \(m\ge0\). The induction is by induction
       on \(m\). We may take \(M_1\) as a \((1,1)\)-eliminator.

       For \(m=1\), the construction of a \((1,2)\)-eliminator is reduced to the construction of
       an \((\alpha+\alpha)\)-eliminator from an \(\alpha\)-eliminator. Given \(a_0>a_1>\dots\),
       apply an \(\alpha\)-eliminator to obtain \(b_0>b_1>\dots\), where \(\{b_i\}\) is
       an \(\alpha\)-sequence, \(b_0\) is the \(\alpha\)-major part of \(a_0\), and if \(\{b_i\}\)
       is finite, then so is \(\{a_i\}\). Each \(b_i\) can be written in the form \(\omega^\alpha \cdot c_i\),
       where \(\{c_i\}\) is decreasing and if \(\{c_i\}\) is finite ,then so
       it \(\{b_i\}\). \(a_0=b_0+e_0\) where \(e_0<\omega^\alpha\). Apply \(\alpha\)-eliminator
       to \(\{c_i\}\) to obtain \(d_0>d_1>\dots\), where \(\{d_i\}\) is
       an \(\alpha\)-sequence, \(d_0\) is the \(\alpha\)-major part of \(c_0\) and if \(\{d_i\}\) is
       finite, then so is \(\{c_i\}\).

       \(\{\omega^\alpha\cdot d_i\}\) is an \((\alpha+\alpha)\)-sequence and decresing.
       If \(\{\omega^\alpha d_i\}\) is finite, then so are \(\{d_i\}\),\(\{c_i\}\), \(\{b_i\}\), \(\{a_i\}\)
       successively, and
       \begin{align*}
    \omega^\alpha\cdot d_0&=\omega^\alpha\cdot (\text{the $\alpha$-major part of }c_0)\\
    &=(\alpha+\alpha)\text{-major part of }b_0\\
    &=(\alpha+\alpha)\text{-major part of }a_0
       \end{align*}
       So \(\{\omega^\alpha d_i\}\) is the \((\alpha+\alpha)\)-sequence which was desired for \(\{a_i\}\)

       For \(m>1\), suppose \(f\) is an \((\alpha,m)\)-eliminator. Then by 12 we can construct
       an \((\alpha\cdot\omega,m)\)-eliminator concretely from \(f\). Hence we have a \((1,m+1)\)-eliminator
   14. Conclusion: An \((\alpha,n)\)-eliminator can be constructed for every \alpha of the form \(\omega_m\),
       i.e.,
       \begin{equation*}
    \begin{rcases}
    \omega^{\cdot^{\cdot^{\cdot^{\omega}}}}
    \end{rcases}
    m
       \end{equation*}
       The construction is by induction on \(m\). If \(m=0\) then we define \alpha to be \(1=\omega^0\).
       Then an \((\alpha,n)\)-eliminator has been defined in 13 for every \(n\). Suppose \(f\)
       is \((1,n)\)-eliminator, and \(g\) is an \((\alpha,n+1)\)-eliminator, which we assume to have been
       defined. Then \(g\) operates on \(f\) and produces the
       required \((1\cdot\omega^\alpha,n)=(\omega^\alpha,n)\)-eliminator.


   An ordinal \mu is *accessible* if it has been demonstrated that every strictly decreasing sequence
   starting with \mu is finite. More precisely, we consider the notion of accessibility only when we
   have actually seen, or demonstrated constructively, that a given ordinal is accessible.

   First, we assume we have arithmetized the construction of the ordinals (less than \(\epsilon_0\))
   given by clauses O1-O3. In other words, we assume a Gödel numbering of these ordinals, with
   certain nice properties: namely, the induced number-theoretic relations and functions
   corresponding to the ordinal relations and functions =,<,+,\(\cdot\) and exponentiation by \omega are
   primitive recursive. also we can primitive recursively represent any (Gödel number of an) ordinal
   in its normal form, and hence decide primitive recursively whether it represents a limit or
   successor ordinal, etc. The ordering of the natural numbers corresponding to < will be called a
   "standard well-ordering of type \(\epsilon_0\)".

   Our method for proving the accessibility of ordinals will be as follows
   1. when it is known that \(\mu_1<\mu_2<\dots\to\nu\) (i.e., \nu is the limit of the increasing
      sequence \(\{\mu_i\}\)) and that every \(\mu_i\) is accessible, then \nu is also accessible
   2. A method is given by which, from the accessibility of a subsystem, one can deduce the
      accessibility of a larger system
   3. by repeating 1 and 2, we show that every initial segment of our ordering is accessible, and
      hence so is the whole ordering


   Consider the decreasing sequences of ordinals less than \(\omega+\omega\). Here we can again see
   that every decreasing sequence terminates. Consider the first term \(\mu_0\) of such a sequence.
   We can effectively decide whether it is of the form \(n\) or of the form \(\omega+n\),
   where \(n\) is a natural number. If it is of the form \(\omega+n\), consider the first \(n+2\)
   terms of the sequence
   \begin{equation*}
    \mu_{n+1}<\dots<\mu_2<\mu_1<\mu_0
   \end{equation*}
   It is easily seen that \(\mu_{n+1}\) cannot be of the form \(\omega+m\) for any natural
   number \(m\) and hence must be a natural number. This method can be extended to the cases of
   decreasing sequences of ordinals less than \(\omega\cdot n\), less than \(\omega^2\), less
   than \(\omega^\omega\), etc


   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   If \mu and \nu are accessible, then so is \(\mu+\nu\)
   #+END_lemma

   #+BEGIN_proof
   given ordinals \mu,\xi,\nu s.t. \(\mu\le\xi<\nu\), we can effectively find a \(\nu_0\) s.t.
   \(\nu_0<\nu\) and \(\xi=\mu+\nu_0\)
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   if \mu is accessible, then so is \(\mu\cdot\omega\)
   #+END_lemma

   #+BEGIN_proof
   If \(\nu<\mu\cdot\omega\), then we can find an \(n\) s.t. \(\nu<\mu\cdot n\)
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   \mu is said to be *1-accessible* if \mu is accessible, \mu is said to be *\((n+1)\)-accessible* if for
   every \nu which is \(n\)-accessible, \(\nu\cdot\omega^\mu\) is \(n\)-accessible
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
    If \mu is \(n\)-accessible and \(\nu<\mu\), then \nu is \(n\)-accessible
   #+END_lemma

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   label:lemma11.5
   Suppose \(\{\mu_m\}\) is an increasing sequence of ordinals with limit \mu. If each \(\mu_m\)
   is \(n\)-accessible, then so is \mu
   #+END_lemma

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
    label:lemma11.6
   If \nu is \((n+1)\)-accessible, then so is \(\nu\cdot\omega\)
   #+END_lemma

   #+BEGIN_proof
   We must show that for any \(n\)-accessible \mu, \(\mu\cdot^{\nu\cdot\omega}\) is \(n\)-accessible.
   For this purpose it suffices to show that \(\mu\cdot\omega^{\nu\cdot m}\) is \(n\)-accessible for
   each \(m\) (cf. Lemma ref:lemma11.5). This is obvious since
   \begin{equation*}
    \mu\cdot\omega^{\nu\cdot m}=\mu\cdot\omega^\nu\dots\omega^\nu
   \end{equation*}
   and \nu is \((n+1)\)-accessible
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   1 is \((n+1)\)-accessible
   #+END_proposition

   #+BEGIN_proof
   Suppose \mu is \(n\)-accessible. Then by Lemma ref:lemma11.6 \(\mu\cdot\omega=\mu\cdot\omega^1\) is \(n\)-accessible
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   \(\omega_0=1\); \(\omega_{n+1}=\omega^{\omega_n}\)
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   \(\omega_k\) is \((n-k)\)-accessible for an arbitrary \(n>k\)
   #+END_proposition

   #+BEGIN_proof
   By induction on \(k\). If \(k=0\), then \(\omega_k=1\) and hence is \(n\)-accessible for
   all \(n\). Suppose \(\omega_k\) is \((n-k)\)-accessible. Since 1 is \([n-(k+1)]\)-accessible,
   \(1\cdot\omega^{\omega_k}\)  is \([n-(k+1)]\)-accessible, i.e., \(\omega_{k+1}\) is \([n-(k+1)]\)-accessible
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop11.10
   \(\omega_k\) is accessible for every \(k\)
   #+END_proposition

   Given any decreasing sequence of ordinals (less than \(\epsilon_0\)) there is an \(\omega_k\)
   s.t. all ordinals in the sequence are less than \(\omega_k\). Therefore the sequence must be
   finite by Proposition ref:prop11.10. Thus we can conclude:

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   \(\epsilon_0\) is accessible
   #+END_proposition

   A Gentzen-style consistency proof is carried out as follows
   1. Construct a suitable standard ordering, in the strictly finitist standpoint
   2. Convince oneself, in the Hilbert-Gentzen standpoint, that it is indeed a well-ordering
   3. othewise use only strictly finitist means in the consistency proof




** A Consistency Proof of \(\PA\)

   We assume from now on that \(\PA\) is formalized in a language which includes a constant \(f\)
   for every primitive recursive function \(f\). We call this language \(L\).

   As initial sequents of \(\PA\) we will also take from now on the defining equations for all
   primitive recursive functions, as well as all sequents \(\to s=t\) where \(s,t\) are closed terms
   of \(L\) denoting the same number, and all sequents \(s=t\to\) where \(s,t\) are closed terms
   of \(L\) denoting different numbers.

   Let \(R\) be a property of proofs s.t.

   (\(\star\)) For any proof \(P\) satisfying \(R\), we can find (effectively from \(P\)) a proof \(P'\)
   satisfying \(R\) s.t. \(P'\) has a smaller ordinal than \(P\)

   We can then infer from (*) and the accessibility of \(\epsilon_0\):

   (\(\star\star\)) No proof satisfies \(R\)

   The procedure of finding \(P'\) from \(P\) in (\(\star\)) is called a *reduction of \(P\) to \(P'\)* (for
   the property \(R\))

   The property \(R\) of proofs that we will be interested in is the property of having \(\to\) as
   an end-sequent

   By giving a uniform reduction procedure for this property (Lemma ref:lemma12.8) we will have
   shown that no proof of \(\PA\) ends with \(\to\), in other words

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   label:thm12.1
   The system \(\PA\) is consistent
   #+END_theorem

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A proof in \(\PA\) is *simple* if no free variables occur in it, and it contains only mathematical
   initial sequents, weak inferences and inessential cuts
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   label:lemma12.3
   There is no simple proof of \(\to\)
   #+END_lemma

   #+BEGIN_proof
   Let \(P\)  be any simple proof. All the formulas in \(P\) are of the form \(s=t\) with \(s\)
   and \(t\) closed.

   A sequent in \(P\) is then given the value \(\sfT\) if at least one formula in the antecedent is
   false or at least one formula in the succedent is true, and it is given the value \(\sfF\). It is
   easy to see  that all mathematical initial sequents take the value \(\sfT\), and weak inferences
   and inessential cuts preserve the value \(\sfT\) downward for sequents. So all sequents of \(P\)
   have the value \(\sfT\). But \(\to\) has the value \(\sfF\).
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   1. The *grade of a formula*, is the number of logical symbols it contains. The *grade of a cut* is
      the grade of the cut formula; the *grade of an ind inferene* is the grade of the induction formula
   2. The *height of a sequent* \(S\) in a proof \(P\) (denoted by \(h(S;P)\) or for short \(h(S)\))
      is the maximum of the grades of the cuts and ind's which occur in \(P\) below \(S\)
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop12.5
   1. The height of the end-sequent of a proof is 0
   2. If \(S_1\) is above \(S_2\) in a proof, then \(h(S_1)\ge h(S_2)\); if \(S_1\) and \(S_2\) are
      the upper sequent of an inference, then \(h(S_1)=h(S_2)\)
   #+END_proposition

   For any ordinal \alpha and natural number \(n\), \(\omega_n(\alpha)\) is defined by induction on \(n\);
   \(\omega_0(\alpha)=\alpha\), \(\omega_{n+1}=\omega^{\omega_n(\alpha)}\). So
   \begin{equation*}
    \omega_n(\alpha)={\underbrace{\omega^{\cdot^{\cdot^{\cdot^{\omega}}}}}_{n}}^{^{\alpha}}
   \end{equation*}

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Assignment of ordinals (less than \(\epsilon_0\)) to the proofs of \(\PA\). First we assign
   ordinals to the sequents in a proof. The ordinal assigned to a sequent \(S\) in a proof \(P\) is
   denoted by \(o(S;P)\) or \(o(S)\). Now suppose a proof \(P\) is given. We shall
   define \(o(S)=o(S;P)\) for all sequents
   #+END_definition

   We assume that the ordinals are expressed in normal form. If \mu and \nu are ordinals of the form
   \(\omega^{\mu_1}+\dots+\omega^{\mu_m}\) and \(\omega^{\nu_1}+\dots+\omega^{\nu_n}\) respectively,
   then \(\mu\#\nu\) denotes the ordinal \(\omega^{\lambda_1}+\dots+\omega^{\lambda_{m+n}}\) where
   \(\{\lambda_1,\dots,\lambda_{m+n}\}=\{\mu_1,\dots,\mu_m,\nu_1,\dots,\nu_n\}\) and
   \(\lambda_1\ge\dots\ge\lambda_{m+n}\). \(\mu\#\nu\) is called the *natural sum* of \mu and \nux

   1. An initial sequent is assigned the ordinal 1
   2. If \(S\) is the lower sequent of a weak inference, then \(o(S)\) is the same as the ordinal of
      its upper sequent.
   3. If \(S\) is the lower sequent
      of \(\wedge\)left,\(\vee\)right,\(\supset\)right,\(\neg\)right,\(\neg\)left or an inference
      involving a quantifier, and tnhe upper sequent has the ordinal \mu, then \(o(S)=\mu+1\)
   4. If \(S\) is the lower sequent of \(\wedge\)right,\(\vee\)left or \(\supset\)left and the upper
      sequents have ordinals \mu and \nu, then \(o(S)=\mu\#\nu\)
   5. If \(S\) is the lower sequent of a cut and its upper sequents have the ordinals \mu and \nu,
      then \(o(S)=\omega_{k-l}(\mu\#\nu)\), i.e.
      \begin{equation*}
    \begin{rcases}
    \omega^{\cdot^{\cdot^{\cdot^{\omega^{\mu\#\nu}}}}}
    \end{rcases}k-l
      \end{equation*}
      where \(k\) and \(l\) are the heights of the upper sequents and of \(S\), respectively
   6. If \(S\) is the lower sequent of an ind and its upper sequent has the ordinal \mu, then \(o(S)\)
      is \(\omega_{k-l+1}(\mu_1+1)\)
      \begin{equation*}
    \begin{rcases}
    \omega^{\cdot^{\cdot^{\cdot^{\omega^{\mu_1+1}}}}}
    \end{rcases}(k-l)+1
      \end{equation*}
      where \mu has the normal form \(\omega^{\mu_1}+\dots+\omega^{\mu_n}\) (so
      that \(\mu_1\ge\mu_2\ge\dots\ge\mu_n\)) and \(k\) and \(l\) are the heights of the upper
      sequent and of \(S\) respectively
   7. The ordinal of a proof \(P\), \(o(P)\) is the ordinal of its end-sequent. We use the notation
      \begin{equation*}
    P:
    \begin{prooftree}%[center=false]
    \hypo{}
    \ellipsis{}{\Gamma\xrightarrow{\mu}\Delta}
    \end{prooftree}
      \end{equation*}
      to denote a proof \(P\) of \(\Gamma\to\Delta\) s.t. \(o(\Gamma\to\Delta;P)=o(P)=\mu\)

      #+ATTR_LATEX: :options []
      #+BEGIN_lemma
      label:lemma12.7
      Suppose \(P\) is a proof containing a sequent \(S_1\) , there is no ind below \(S_1\), \(P_1\)
      is the subproof of \(P\) ending with \(S_1\), \(P_1'\) is any other proof of \(S_1\)
      and \(P'\) is the proof formed from \(P\) by replacing \(P_1\) by \(P_1'\)
      \begin{equation*}
    P:
    \begin{prooftree}%[center=false]
    \hypo{}
    \ellipsis{\(P_1\)}{S_1}
    \ellipsis{}{}
    \end{prooftree}\hspace{1cm}
    P':
    \begin{prooftree}%[center=false]
    \hypo{}
    \ellipsis{\(P_1'\)}{S_1}
    \ellipsis{}{}
    \end{prooftree}
      \end{equation*}
      Suppose also that \(o(S_1;P')<o(S_1;P)\). Then \(o(P')<o(P)\)
      #+END_lemma

      #+BEGIN_proof
      Consider a thread of \(P\) passing through \(S_1\). We show that for any sequent \(S\) of this
      thread at or below \(S_1\): if \(S'\) is the sequent "corresponding to" \(S\) in \(P'\), then
      \begin{equation*}
    o(S';P')<o(S;P)
      \end{equation*}
      This is true for \(S=S_1\) by assumption, and this property is preserves downwards by all the
      inference rules (We use the fact that the natural sum is strictly monotonic in each argument,
      i.e., \(\alpha<\beta\Rightarrow\alpha\#\gamma<\beta\#\gamma\)). Finally let \(S\) be the end-sequent
      #+END_proof

      Now let \(R\) be the property of proofs of ending with the sequent \(\to\); i.e., for any
      proof \(P\), \(R(P)\) holds iff \(P\) is a proof of \(\to\)

      Notice first that if \(P\) is a proof of \(\to\), then every logical inference of \(P\) is
      implicit (cf. Definition ref:def9.7). Hence the definition of end-piece for such proofs can be
      simply stated as follows.

      The end-piece of a proof of \(\to\) consists of all those sequents that are encountered as we
      acsend each thread from the end-sequent and stop as soon as we arrive at a logical inference.
      This inference belongs to the boundary

      #+ATTR_LATEX: :options []
      #+BEGIN_lemma
      label:lemma12.9
      If \(P\) is a proof of \(\to\), then there is another proof \(P'\) of \(\to\) s.t. \(o(P')<o(P)\)
      #+END_lemma

      #+BEGIN_proof
      Let \(P\) be a proof of \(\to\). We can assume, by Proposition ref:prop9.8 that \(P\) is
      regular. We describe a "reduction" of \(P\) to obtain the desired \(P'\).

      At each step, the ordinal of the resulting proof does not increase, and at least at one step,
      the ordinal decreases

      /Step 1/. Suppose the end-piece of \(P\) contains a free variable, say \(a\), which is not used
      as an eigenvariable. Then replace \(a\) by constant 0. This results in a proof of \(\to\),
      with the same ordinal.

      Step 1 is performed repeated until there is no free variable in the end-piece which is not
      used as an eigenvariable

      /Step 2/. Suppose the end-sequence of \(P\) contains an ind. Then take a lowermost one,
      say \(I\). Suppose \(I\) is of the following form
      \begin{equation*}
    \begin{prooftree}%[center=false]
    \hypo{}
    \ellipsis{\(P_0(a)\)}{F(a),\Gamma\xrightarrow{\mu}\Delta,F(a')}
    \infer1[\(I\)]{F(0),\Gamma\to\Delta,F(s)}
    \ellipsis{}{\to}
    \end{prooftree}
      \end{equation*}
      where \(P_0\) is the subproof ending with \(F(a),\Gamma\to\Delta,F(a')\) and let \(l\) and \(k\) be
      the heights of the upper sequent (call it \(S\)) and the lower sequent (calll it \(S_0\))
      of \(I\), respectively. Then
      \begin{equation*}
    o(S_0)=\omega_{l-k+1}(\mu_1+1)
      \end{equation*}
      where \(\mu=o(S)=\omega^{\mu_1}+\dots+\omega^{\mu_n}\) and \(\mu_n\le\dots\le\mu_1\). Since no
      free variable occurs below \(I\), \(s\) is a closed term and hence there is a number \(m\)
      s..t \(\to s=\barm\) is \(\PA\)-provable without an essential cut or ind (cf. Lemma
      ref:lemma9.6). Hence there is a proof \(Q\) of \(F(\barm)\to F(s)\) without an essential cut or
      ind (cf. Lemma ref:lemma9.6).
      Let \(P_0(\barn)\) be the proof which is obtained from \(P_0\) by
      replacing \(a\) by \(\barn\) throughout. Consider the following proof \(P'\)
      \begin{equation*}
    \begin{prooftree}%[center=false]
    \hypo{P_0(\bar{0})}
    \ellipsis{}{S_1:F(0),\Gamma\to\Delta,F(0')}
    \hypo{P_0(\bar{1})}
    \ellipsis{}{F(0'),\Gamma\to\Delta,F(0'')}
    \infer2{S_2: F(0),\Gamma\to\Delta,F(0'')}
    \hypo{P_0(\bar{2})}
    \ellipsis{}{F(0''),\Gamma\to\Delta,F(0''')}
    \infer2{S_3: F(0),\Gamma\to\Delta,F(0''')}
    \end{prooftree}
      \end{equation*}
      \begin{equation*}
    \begin{prooftree}%[center=false]
    \hypo{}
    \ellipsis{}{S_m:F(0),\Gamma\to\Delta,F(\barm)}
    \hypo{Q}
    \ellipsis{}{F(\barm)\to F(s)}
    \infer2{S_0:F(0),\Gamma\to\Delta,F(s)}
    \ellipsis{}{\to}
    \end{prooftree}
      \end{equation*}
      where \(S_1,S_2,\dots,S_0\) denotes the sequents shown on their right, \(S_1,\dots,S_m\) all
      have height \(l\), since the formulas \(F(\barn)\), \(n=0,\dots,m\) all have the same grade.
      Therefore
      \begin{equation*}
    o(F(\barn),\Gamma\to\Delta,F(\barn');P')=\mu\hspace{1cm}\text{for }n=0,1,\dots,m
      \end{equation*}
      Since \(Q\) has no essential cut or
      ind,
      \(o(F(\barm)\to F(s);P')=q<\omega\), \(o(S_2)=\mu\#\mu\), \(o(S_3)=\mu\#\mu\#\mu\); \(\dots\),
      and in general, writing \(\mu*n=\mu\#\dots\#\mu\) (\(n\) times), \(o(S_n)=\mu*n\)
      for \(n=1,2,\dots,m\). Thus
      \begin{equation*}
    o(S_0)=\omega_{l-k}(\mu*n+q)
      \end{equation*}
      and \(\mu*m+q<\omega^{\mu_1+1}\), since \(q<\omega\). Therefore
      \begin{equation*}
    o(S_0;P')=\omega_{l-k}(\mu*m+q)<\omega_{l-k+1}(\mu_1)=o(S_0;P)
      \end{equation*}
      Thus \(o(S_0;P')<o(S_0;P)\). Hence by Lemma ref:lemma12.7 \(o(P')<o(P)\)

      Thus if \(P\) has an ind the end-piece, we are done; we have reduced \(P\) to a proof \(P'\)
      of \(\to\) with \(o(P')<o(P)\). Otherwise we assume from now on that \(P\) has no ind in its
      end-piece, and go to Step 3.

      /Step 3/. Suppose the end-piece of \(P\) contains a logical initial sequent \(D\to D\). Since
      the end-sequent is empty, both \(D\)'s (or more strictly, descendants of both \(D\)'s) must
      disappear by cuts. Suppose that (a descendant of) the \(D\) in the antecedent is a cut formula
      first (viz. in the following figure a descendant of the \(D\) in the succedent of \(D\to D\)
      occurs in \Xi)
      \begin{equation*}
    \begin{prooftree}%[center=false]
    \hypo{}
    \ellipsis{}{\Gamma \to \Delta,D}
    \hypo{D\to D}
    \ellipsis{}{D,\Pi \to \Xi}
    \infer2{S:\Gamma,\Pi \to \Delta,\Xi}
    \ellipsis{}{\to}
    \end{prooftree}
      \end{equation*}
      \(P\) is reduced to the following \(P'\):
      \begin{equation*}
    \begin{prooftree}%[center=false]
    \hypo{}
    \ellipsis{}{\Gamma \to \Delta,D}
    \infer1{\text{weakenings and exchanges}}
    \infer1{S':\Gamma,\Pi \to \Delta,\Xi}
    \ellipsis{}{\to}
    \end{prooftree}
      \end{equation*}
      Note that there is a cut whose cut formula is \(D\) below \(S\) since both \(D\)s
      in \(D\to D\) must disappear by cuts. Hence the height of \(\Gamma \to \Delta,D\) does not change when we
      transform \(P\) into \(P'\): \(o(S';P')<o(S;P)\).

      Hence by Lemma ref:lemma12.7, \(o(P')<o(P)\)

      We assume from now on that the end-piece of \(P\) contains no logical initial sequents

      /Step 4/. Suppose there is a weakening in the end-piece. Let \(I\) be the lower most weakening
      inference in the end-piece. Since the end-sequent is empty, there must exists a cut, \(J\),
      below \(I\) and the cut formula is the descendent of the principal formula of \(I\).
      \begin{equation*}
    \begin{prooftree}%[center=false]
    \hypo{}
    \ellipsis{}{\Gamma \to \Delta,D}
    \hypo{}
    \ellipsis{}{\Pi' \to \Xi'}
    \infer1[\(I\)]{D,\Pi'\to\Xi'}
    \ellipsis{}{D,\Pi\to\Xi\quad(k)}
    \infer2[\(J\)]{\Gamma,\Pi\to\Delta,\Xi \quad(l)}
    \ellipsis{}{\to}
    \end{prooftree}
      \end{equation*}
      1. If no contraction is applied to \(D\) from the inference \(I\)  through \(J\), by deleting
         some exchanges from \(P\) if necessary, reduce \(P\) into the following proof \(P'\)
         \begin{equation*}
         \begin{prooftree}%[center=false]
         \hypo{}
         \ellipsis{}{\Pi'\to\Xi'}
         \ellipsis{}{\Pi \to \Xi \quad(l)}
         \infer1[]{\text{weakenings and exchanges}}
         \infer1{\Gamma,\Pi \to \Delta,\Xi \quad(l)}
         \ellipsis{}{\to}
         \end{prooftree}
         \end{equation*}
         Let \(h(\Gamma,\Pi \to \Delta,\Xi;P)=l\) and \(h(D,\Pi \to \Xi;P)=k\). Then \(l\le k\)
         and \(h(\Pi\to\Xi;P')=h(\Gamma,\Pi\to\Delta,\Xi;P')=l\). Let \(S\) be a sequent in \(P\)
         above \(D,\Pi\to\Xi\) and let \(S'\) be the corresponding sequent in \(P'\). Then by the
         induction on number of inferences up to \(D,\Pi\to\Xi\), we can show that
         \begin{equation*}
         \omega_{k_1-k_2}(o(S;P))\ge o(S';P')
         \end{equation*}
         where \(k_1=h(S;P)\) and \(k_2=h(S';P)\) (<<Problem4>>). Hence if \(o(\Gamma \to \Delta,D;P)=\mu_1\),
         \(o(D,\Pi\to\Xi;P)=\mu_2\), \(o(\Gamma,\Pi\to\Delta,\Xi)=\nu\), \(o(\Pi\to\Xi;P)=\mu_2'\)
         and \(o(\Gamma,\Pi\to\Delta,\Xi)=\nu'\), then
         \begin{equation*}
         \omega_{k-l}(\mu_2)\ge\mu_2'
         \end{equation*}
         and further
         \begin{equation*}
         \nu=\omega_{k-l}(\mu_2\#\mu_1)>\omega_{k-l}(\mu_2)\ge\mu_2'=\nu'
         \end{equation*}
         Thus \(o(P)>o(P')\)
      2. If not the case 1, let the uppermost contraction applied to \(D\) be \(I'\). Reduce \(P\)
         into the following proof \(Q\):
         \begin{equation*}
         P:
         \begin{prooftree}%[center=false]
         \hypo{}
         \ellipsis{}{\Pi'\to\Xi'}
         \infer1{D,\Pi'\to\Xi'}
         \ellipsis{}{D,D,\Pi''\to\Xi''}
         \infer1{D,\Pi''\to\Xi''}
         \ellipsis{}{D,\Pi\to\Xi}
         \end{prooftree}\hspace{1cm}
         Q:
         \begin{prooftree}%[center=false]
         \hypo{}
         \ellipsis{}{\Pi'\to\Xi'}
         \ellipsis{}{D,\Pi''\to\Xi''}
         \ellipsis{}{D,\Pi\to\Xi}
         \end{prooftree}
         \end{equation*}
         Apparently, \(o(P)=o(Q)\). Hence we can assume that the end-piece of \(P\), contains no
         weakening.


      /Step 5/. We can now assume that \(P\) is not its own end-piece, since otherwise it would be
      simple, and hence by Lemma ref:lemma12.3 could not end with \(\to\)
      #+END_proof

      #+ATTR_LATEX: :options [sublemma]
      #+BEGIN_lemma
      Suppose that a proof in \(\PA\), say \(P\), satisfies the following
      1. \(P\) is not its own end-piece
      2. The end-piece of \(P\) does not contain any logical inference, ind or weakening
      3. If an initial sequent belongs to the end-piece of \(P\), then it does not contain any
         logical symbol


      Then there exists a suitable cut in the end-piece of \(P\)
      #+END_lemma

      #+BEGIN_proof
    Induction on the number of essential cuts in the end-piece of \(P\)

    The end-piece of \(P\) contains an essential cut (<<Problem5>>), since \(P\) is not its own
    end-piece. Take a lowermost such cut, say \(I\). If \(I\) is a suitable cut, then the lemma is
    proved. Otherwise, let \(P\) be of the form
    \begin{equation*}
    \begin{prooftree}%[center=false]
    \hypo{}
    \ellipsis{\(P_1\)}{\Gamma \to \Delta,D}
    \hypo{}
    \ellipsis{\(P_2\)}{D,\Pi \to \Lambda}
    \infer[left label=\(I\)]2{\Gamma,\Pi \to \Delta,\Lambda}
    \end{prooftree}
    \end{equation*}
    Since \(I\) is not a suitable cut, one of two cut formulas of \(I\) is not a descendant of the
    principal formula of a boundary inference. Suppose that \(D\) in \(\Gamma \to \Delta,D\) is not a descendant
    of the principal formula of a boundary inference. Now we prove:
    1. \(P_1\) contains a boundary inference of \(P\)

       Suppose otherwise. Then \(D\) in \(\Gamma \to \Delta,D\) is a descendant of \(D\) in an initial sequent
       in the end-piece of \(P\) by 2. This contradicts the assumption that \(I\) is an essential
       cut by 3.

    2. If an inference \(J\) in \(P_1\) is a boundary inference of \(P\), then \(J\) is a boundary
       inference of \(P_1\)

    3. \(P_1\) is not its own end-piece and the end-piece of \(P_1\) is the intersection of \(P_1\)
       and the end-piece of \(P\)

       this follows from 1 and 1,2 (condition first)


    Then end-piece of \(P_1\) has a suitable cut. This cut is suitable cut in the end-piece of \(P\).
      #+END_proof

      #+ATTR_LATEX: :options [\cite{lemma12.8}]
      #+BEGIN_proof
      Returning to our proof \(P\) of \(\to\) which satisfies the conclusion of steps 1-4, we have,
      as an immediate consequence of Sublemma ref:lemma12.9 that the end-piece of \(P\) contains a
      suitable cut. We now define an *essential reduction* of \(P\)

      Take a lowermost suitable cut in the end-piece of \(P\), say \(I\)

      Case 1. The cut formula of \(I\) is of the form \(A\wedge B\). Suppose \(P\) is of the form
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{}
      \ellipsis{}{\Gamma \to \Theta',A}
      \hypo{}
      \ellipsis{}{\Gamma\to\Theta',B}
      \infer[left label=\(I_1\)]2{\Gamma'\to\Theta',A\wedge B}
      \ellipsis{}{\Gamma\xrightarrow{\mu}\Theta,A\wedge B}
      \hypo{A,\Pi'\to\Lambda'}
      \infer[left label=\(I_2\)]1{A\wedge B,\Pi'\to\Lambda'}
      \ellipsis{}{A\wedge B,\Pi\xrightarrow{\nu}\Lambda\quad (l)}
      \infer[left label=\(I\)]2{\Gamma,\Pi \to \Theta,\Lambda}
      \ellipsis{}{\Delta\xrightarrow{\lambda}\Xi\quad(k)}
      \ellipsis{}{\to}
      \end{prooftree}
      \end{equation*}
      where \(\Delta \to \Xi\) denotes the uppermost sequent below \(I\) whose height is less than that of
      the upper sequents of \(I\). Let \(l\) be the height of each upper sequent of \(I\), and \(k\)
      that of \(\Delta \to \Xi\). Then \(k<l\). Note that \(\Delta \to \Xi\) may be the lower sequent of \(I\), or
      the end-sequent. The existence of such a sequent follows from Proposition ref:prop12.5

      \(\Delta \to \Xi\) must be the lower sequent of a cut \(J\) (since there is no ind below \(I\)).
      Consider the following proofs
      \begin{equation*}
      P_1:\hspace{1cm}
      \begin{prooftree}%[center=false]
      \hypo{}
      \ellipsis{}{\Gamma'\to\Theta',A}
      \infer[double]1{\Gamma'\to A,\Theta'}
      \infer1[(weakening:right)]{\Gamma'\to A,\Theta',A\wedge B}
      \ellipsis{}{\Gamma\xrightarrow{\mu_1}A,\Theta,A\wedge B}
      \hypo{}
      \ellipsis{}{A\wedge B,\Pi \xrightarrow{\nu_1}\Lambda\quad(l)}
      \infer[left label=\(J_1\)]2{\Gamma,\Pi \to A,\Theta,\Lambda}
      \ellipsis{}{\Delta \xrightarrow{\lambda_1}A,\Xi}
      \infer[double]1[\((m)\)]{\Delta \to \Xi,A}
      \end{prooftree}
      \end{equation*}
      \begin{equation*}
      P_2:\hspace{1cm}
      \begin{prooftree}%[center=false]
      \hypo{}
      \ellipsis{}{\Gamma \xrightarrow{\mu_2}\Theta,A\wedge B}
      \hypo{}
      \ellipsis{}{A,\Pi' \to \Lambda'}
      \infer[double]1{\Pi',A\to \Lambda'}
      \infer1{A\wedge B,\Pi',A\to\Lambda'}
      \ellipsis{}{A\wedge B,\Pi,A\xrightarrow{\nu_2}\Lambda}
      \infer[left label=\(J_2\)]2{\Gamma,\Pi,A\to\Theta,\Lambda}
      \ellipsis{}{\Delta,A\xrightarrow{\lambda_2}\Xi}
      \infer[double] 1[\((m)\)]{A,\Delta \to \Xi}
      \end{prooftree}
      \end{equation*}
      (where \(l\) and \(m\) are the heights of the sequents shown, not in \(P_1\) and \(P_2\), but
      in \(P'\), defined below, which contains these as subproofs)

      Define \(P'\) to be the proof
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{}
      \ellipsis{\(P_1\)}{\Delta\xrightarrow{\lambda_1}\Xi,A\quad (m)}
      \hypo{}
      \ellipsis{\(P_2\)}{A,\Delta\xrightarrow{\lambda_2}\Xi\quad(m)}
      \infer2[\(I'\),cut for \(A\)]{\Delta,\Delta \xrightarrow{\lambda_0}\Xi,\Xi\quad(k)}
      \infer[double]1{\Delta \to \Xi}
      \ellipsis{}{\to}
      \end{prooftree}
      \end{equation*}
      So \(m\) is the height of the upper sequents of \(I'\). Note that the height of the lower
      sequent of \(I'\) is \(k\)

      It is obvious that \(m=k\) if \(k>\) grade of \(A\) and \(m=\) grade of \(A\) otherwise. In
      either case \(k\le m<l\)
      \begin{equation*}
      h(\Gamma\to A,\Theta,A\wedge B;P')=h(A\wedge B,\Pi\to\Lambda;P')=l
      \end{equation*}
      since all cut formulas below \(I\) in \(P\) occur in \(P'\) below \(J_1\), all cut formulas
      below \(J_1\) in \(P'\) except \(A\) occur in \(P\) under \(I\), and grade of \(A\) < grade
      of \(A\wedge B\le l\). Similarly
      \begin{equation*}
      h(\Gamma\to\Theta,A\wedge B;P')=h(A\wedge B,\Pi,A\to\Lambda;P')=l
      \end{equation*}

      Also \(\mu_1<\mu,\nu_1=\nu,\mu_2=\mu\) and \(\nu_2<\nu\)

      Now let
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{S_1'}
      \hypo{S_2'\quad(k_1)}
      \infer[left label=\(J'\)]2{S'\quad(k_2)}
      \end{prooftree}
      \end{equation*}
      be an arbitrary inference between \(J_1\) and \(\Delta\to A,\Xi\) and let
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{S_1}\hypo{S_2}
      \infer[left label=\(J\)]2{S}
      \end{prooftree}
      \end{equation*}
      be the corresponding inference between \(I\) and \(\Delta\to\Xi\). Let
      \begin{gather*}
      \alpha_1'=o(S_1';P'),\quad\alpha_2'=o(S_2';P'),\quad\alpha'=o(S';P')\\
      \alpha_1=o(S_1;P),\quad\alpha_2=o(S_2;P),\quad\alpha=o(S;P)\\
      k_1=h(S_1';P')=h(S_2';P'),\quad k_2=h(S',P')
      \end{gather*}
      The \(\alpha=\alpha_1\#\alpha_2\) if \(S\) is not \(\Delta\to A,\Xi\) and \(\alpha=\omega_{l-k}(\alpha_1\#\alpha_2)\) if \(S'\) is \(\Delta\to A,\Xi\)
      (as \(\Delta\to A,\Xi\) is the lower sequent of a cut).
      On the other hand \(\alpha'=\omega_{k_1-k_2}(\alpha_1'\#\alpha_2')\)

      Starting with \(\mu_1<\mu\) and \(\nu_1=\nu\) it is easily seen by induction on the number of inferences
      between \(J_1\) and \(S\) that
      \begin{equation*}
      \alpha'<\omega_{l-k_2}(\alpha)\tag{\(\star\)}
      \end{equation*}
      if \(S\) is not \(\Delta\to A,\Xi\) (<<Problem6>>). Let \(\lambda=\omega_{l-k}(\kappa)\). Then (\(\star\)) implies that
      \(\lambda_1<\omega_{l-m}(\kappa)\). Similar. Similarly \(\lambda_2<\omega_{l-m}(\kappa)\). Hence
      \begin{equation*}
      \omega_{m-k}(\lambda_1\#\lambda_2)<\omega_{l-k}(\kappa)
      \end{equation*}
      since \(l-k=(l-m)+(m-k)\). Therefore \(\lambda_0<\lambda\). Finally from \(\lambda_0<\lambda\) it follows
      that \(o(P')<o(P)\)

      Case 2. The cut formula of \(I\) is of the form \(\forall xF(x)\). So \(P\) has the form
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{}
      \ellipsis{}{\Gamma'\to\Theta',F(a)}
      \infer[left label=\(I_1\)]1{\Gamma'\to\Theta',\forall xF(x)}
      \ellipsis{}{\Gamma\to\Theta,\forall xF(x)}
      \hypo{}
      \ellipsis{}{F(s),\Pi'\to\Lambda'}
      \infer[left label=\(I_2\)]1{\forall xF(x),\Pi'\to\Lambda'}
      \ellipsis{}{\forall xF(x),\Pi\to\Lambda}
      \infer[left label=\(I\)]2{\Gamma,\Pi\to\Theta,\Lambda}
      \ellipsis{}{\Delta\to\Xi}
      \ellipsis{}{\to}
      \end{prooftree}
      \end{equation*}
      The definition of \(\Delta\to\Xi\) is the same as in case 1. The proof \(P'\) is then defined in terms
      of the following two subproofs \(P_1\) and \(P_2\)
      \begin{equation*}
      P_1:\hspace{1cm}
      \begin{prooftree}%[center=false]
      \hypo{\Gamma'\to\Theta',F(s)}
      \infer[double]1{\Gamma'\to F(s),\Theta'}
      \infer1{\Gamma'\to F(s),\Theta',\forall xF(x)}
      \ellipsis{}{\Gamma\to F(s),\Theta,\forall xF(x)}
      \hypo{}
      \ellipsis{}{\forall xF(x),\Pi\to\Lambda}
      \infer2{\Gamma,\Pi\to F(s),\Theta,\Lambda}
      \ellipsis{}{\Delta\to F(s),\Xi}
      \infer[double]1{\Delta\to\Xi,F(s)}
      \end{prooftree}
      \end{equation*}

      \begin{equation*}
      P_2:\hspace{1cm}
      \begin{prooftree}%[center=false]
      \hypo{}
      \ellipsis{}{\Gamma\to\Theta,\forall xF(x)}
      \hypo{}
      \ellipsis{}{F(s),\Pi'\to\Lambda'}
      \infer[double]1{\Pi',F(s)\to\Lambda'}
      \infer1{\forall xF(x),\Pi',F(s)\to\Lambda'}
      \ellipsis{}{\forall xF(x),\Pi,F(s)\to\Lambda}
      \infer2{\Gamma,\Pi,F(s)\to\Theta,\Lambda}
      \ellipsis{}{\Delta,F(s)\to\Xi}
      \infer[double]1{F(s),\Delta\to\Xi}
      \end{prooftree}
      \end{equation*}
      \(P'\) is defined to be
      \begin{equation*}
      \begin{prooftree}%[center=false]
      \hypo{}
      \ellipsis{\(P_1\)}{\Delta\to\Xi,F(s)}
      \hypo{}
      \ellipsis{\(P_2\)}{F(s),\Delta\to\Xi}
      \infer2{\Delta,\Delta\to\Xi,\Xi}
      \infer[double]1{\Delta\to\Xi}
      \ellipsis{}{\to}
      \end{prooftree}
      \end{equation*}
      Note that \(o(\Gamma'\to\Theta',F(s);P')=o(\Gamma'\to\Theta',F(a);P)\)

      For the other cases, the proof is similar
      #+END_proof

      #+BEGIN_remark
      Gentzen's consistency proof of \(\PA\) can be formalized in the system of primitive recursive
      arithmetic, together with the axiom \(TI(\prec,F(x))\) where \(\prec\) is the standard well-ordering
      of type \(\epsilon_0\) and \(F(x)\) is a certain quantifier-free formula
   #+END_remark


* TODO ALL the problems

   [[Problem2]]
   [[Problem3]]
   [[Problem4]]

   [[Problem5]] it has a cut, but why this cut is essential
   [[Problem6]] maybe the basic idea is that we only need to consider cut
